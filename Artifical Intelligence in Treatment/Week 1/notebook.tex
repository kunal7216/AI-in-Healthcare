
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{C3M1\_Assignment}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \hypertarget{estimating-treatment-effect-using-machine-learning}{%
\section{Estimating Treatment Effect Using Machine
Learning}\label{estimating-treatment-effect-using-machine-learning}}

    Welcome to the first assignment of \textbf{AI for Medical Treatment}!

You will be using different methods to evaluate the results of a
\href{https://en.wikipedia.org/wiki/Randomized_controlled_trial}{randomized
control trial} (RCT).

\textbf{You will learn:} - How to analyze data from a randomized control
trial using both: - traditional statistical methods - and the more
recent machine learning techniques - Interpreting Multivariate Models -
Quantifying treatment effect - Calculating baseline risk - Calculating
predicted risk reduction - Evaluating Treatment Effect Models -
Comparing predicted and empirical risk reductions - Computing
C-statistic-for-benefit - Interpreting ML models for Treatment Effect
Estimation - Implement T-learner

    \hypertarget{this-assignment-covers-the-folowing-topics}{%
\subsubsection{This assignment covers the folowing
topics:}\label{this-assignment-covers-the-folowing-topics}}

\begin{itemize}
\tightlist
\item
  Section \ref{1}

  \begin{itemize}
  \tightlist
  \item
    Section \ref{1-1}
  \item
    Section \ref{1-2}

    \begin{itemize}
    \tightlist
    \item
      Section \ref{ex-01}
    \item
      Section \ref{ex-02}
    \end{itemize}
  \end{itemize}
\item
  Section \ref{2}

  \begin{itemize}
  \tightlist
  \item
    Section \ref{2-1}

    \begin{itemize}
    \tightlist
    \item
      Section \ref{ex-03}
    \end{itemize}
  \item
    Section \ref{2-2}

    \begin{itemize}
    \tightlist
    \item
      Section \ref{ex-04}
    \end{itemize}
  \item
    Section \ref{2-3}

    \begin{itemize}
    \tightlist
    \item
      Section \ref{ex-05}
    \item
      Section \ref{ex-06}
    \end{itemize}
  \end{itemize}
\item
  Section \ref{3}

  \begin{itemize}
  \tightlist
  \item
    Section \ref{3-1}

    \begin{itemize}
    \tightlist
    \item
      Section \ref{ex-07}
    \item
      Section \ref{ex-08}
    \end{itemize}
  \end{itemize}
\item
  Section \ref{4}

  \begin{itemize}
  \tightlist
  \item
    Section \ref{4-1}

    \begin{itemize}
    \tightlist
    \item
      Section \ref{ex-09}
    \item
      Section \ref{ex-10}
    \item
      Section \ref{ex-11}
    \end{itemize}
  \end{itemize}
\end{itemize}

    \hypertarget{packages}{%
\subsection{Packages}\label{packages}}

We'll first import all the packages that we need for this assignment.

\begin{itemize}
\tightlist
\item
  \texttt{pandas} is what we'll use to manipulate our data
\item
  \texttt{numpy} is a library for mathematical and scientific operations
\item
  \texttt{matplotlib} is a plotting library
\item
  \texttt{sklearn} contains a lot of efficient tools for machine
  learning and statistical modeling
\item
  \texttt{random} allows us to generate random numbers in python
\item
  \texttt{lifelines} is an open-source library that implements
  c-statistic
\item
  \texttt{itertools} will help us with hyperparameters searching
\end{itemize}

\hypertarget{import-packages}{%
\subsection{Import Packages}\label{import-packages}}

Run the next cell to import all the necessary packages, dependencies and
custom util functions.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}1}]:} \PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd}
        \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
        \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
        \PY{k+kn}{import} \PY{n+nn}{sklearn}
        \PY{k+kn}{import} \PY{n+nn}{random}
        \PY{k+kn}{import} \PY{n+nn}{lifelines}
        \PY{k+kn}{import} \PY{n+nn}{itertools}
        
        \PY{n}{plt}\PY{o}{.}\PY{n}{rcParams}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{figure.figsize}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{7}\PY{p}{]}
\end{Verbatim}


     \#\# 1 Dataset \#\#\# 1.1 Why RCT?

In this assignment, we'll be examining data from an RCT, measuring the
effect of a particular drug combination on colon cancer. Specifically,
we'll be looking the effect of
\href{https://en.wikipedia.org/wiki/Levamisole}{Levamisole} and
\href{https://en.wikipedia.org/wiki/Fluorouracil}{Fluorouracil} on
patients who have had surgery to remove their colon cancer. After
surgery, the curability of the patient depends on the remaining residual
cancer. In this study, it was found that this particular drug
combination had a clear beneficial effect, when compared with
\href{https://en.wikipedia.org/wiki/Chemotherapy}{Chemotherapy}. \#\#\#
1.2 Data Processing In this first section, we will load in the dataset
and calculate basic statistics. Run the next cell to load the dataset.
We also do some preprocessing to convert categorical features to one-hot
representations.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}2}]:} \PY{n}{data} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{levamisole\PYZus{}data.csv}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{index\PYZus{}col}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
\end{Verbatim}


    Let's look at our data to familiarize ourselves with the various fields.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3}]:} \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Data Dimensions: }\PY{l+s+si}{\PYZob{}data.shape\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n}{data}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Data Dimensions: (607, 14)

    \end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}3}]:}    sex  age  obstruct  perfor  adhere  nodes  node4  outcome  TRTMT  \textbackslash{}
        1    1   43         0       0       0    5.0      1        1   True   
        2    1   63         0       0       0    1.0      0        0   True   
        3    0   71         0       0       1    7.0      1        1  False   
        4    0   66         1       0       0    6.0      1        1   True   
        5    1   69         0       0       0   22.0      1        1  False   
        
           differ\_2.0  differ\_3.0  extent\_2  extent\_3  extent\_4  
        1           1           0         0         1         0  
        2           1           0         0         1         0  
        3           1           0         1         0         0  
        4           1           0         0         1         0  
        5           1           0         0         1         0  
\end{Verbatim}
            
    Below is a description of all the fields (one-hot means a different
field for each level): -
\texttt{sex\ (binary):\ 1\ if\ Male,\ 0\ otherwise} -
\texttt{age\ (int):\ age\ of\ patient\ at\ start\ of\ the\ study} -
\texttt{obstruct\ (binary):\ obstruction\ of\ colon\ by\ tumor} -
\texttt{perfor\ (binary):\ perforation\ of\ colon} -
\texttt{adhere\ (binary):\ adherence\ to\ nearby\ organs} -
\texttt{nodes\ (int):\ number\ of\ lymphnodes\ with\ detectable\ cancer}
- \texttt{node4\ (binary):\ more\ than\ 4\ positive\ lymph\ nodes} -
\texttt{outcome\ (binary):\ 1\ if\ died\ within\ 5\ years} -
\texttt{TRTMT\ (binary):\ treated\ with\ levamisole\ +\ fluoroucil} -
\texttt{differ\ (one-hot):\ differentiation\ of\ tumor} -
\texttt{extent\ (one-hot):\ extent\ of\ local\ spread}

    In particular pay attention to the \texttt{TRTMT} and \texttt{outcome}
columns. Our primary endpoint for our analysis will be the 5-year
survival rate, which is captured in the \texttt{outcome} variable.

     \#\#\# Exercise 01

Since this is an RCT, the treatment column is randomized. Let's warm up
by finding what the treatment probability is.

\[p_{treatment} = \frac{n_{treatment}}{n}\]

\begin{itemize}
\tightlist
\item
  \(n_{treatment}\) is the number of patients where
  \texttt{TRTMT\ =\ True}
\item
  \(n\) is the total number of patients.
\end{itemize}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}96}]:} \PY{c+c1}{\PYZsh{} UNQ\PYZus{}C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)}
         \PY{k}{def} \PY{n+nf}{proportion\PYZus{}treated}\PY{p}{(}\PY{n}{df}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{    Compute proportion of trial participants who have been treated}
         
         \PY{l+s+sd}{    Args:}
         \PY{l+s+sd}{        df (dataframe): dataframe containing trial results. Column}
         \PY{l+s+sd}{                      \PYZsq{}TRTMT\PYZsq{} is 1 if patient was treated, 0 otherwise.}
         \PY{l+s+sd}{  }
         \PY{l+s+sd}{    Returns:}
         \PY{l+s+sd}{        result (float): proportion of patients who were treated}
         \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
             
             \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} START CODE HERE (REPLACE INSTANCES OF \PYZsq{}None\PYZsq{} with your code) \PYZsh{}\PYZsh{}\PYZsh{}}
         
             \PY{n}{proportion} \PY{o}{=}\PY{n}{df}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{TRTMT}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{p}{)} \PY{o}{/} \PY{n}{df}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
             
             \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} END CODE HERE \PYZsh{}\PYZsh{}\PYZsh{}}
         
             \PY{k}{return} \PY{n}{proportion}
\end{Verbatim}


    \textbf{Test Case}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}97}]:} \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{dataframe:}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{example\PYZus{}df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{data} \PY{o}{=}\PY{p}{[}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}
                                          \PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} 
                                          \PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}
                                          \PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{]}\PY{p}{,} \PY{n}{columns} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{outcome}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{TRTMT}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{example\PYZus{}df}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{treated\PYZus{}proportion} \PY{o}{=} \PY{n}{proportion\PYZus{}treated}\PY{p}{(}\PY{n}{example\PYZus{}df}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Proportion of patient treated: computed }\PY{l+s+si}{\PYZob{}treated\PYZus{}proportion\PYZcb{}}\PY{l+s+s2}{, expected: 0.75}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
dataframe:

   outcome  TRTMT
0        0      0
1        1      1
2        1      1
3        1      1


Proportion of patient treated: computed 0.75, expected: 0.75

    \end{Verbatim}

    Next let's run it on our trial data.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{n}{p} \PY{o}{=} \PY{n}{proportion\PYZus{}treated}\PY{p}{(}\PY{n}{data}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Proportion Treated: }\PY{l+s+si}{\PYZob{}p\PYZcb{}}\PY{l+s+s2}{ \PYZti{} }\PY{l+s+s2}{\PYZob{}}\PY{l+s+s2}{int(p*100)\PYZcb{}}\PY{l+s+s2}{\PYZpc{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]

        ---------------------------------------------------------------------------

        TypeError                                 Traceback (most recent call last)

        <ipython-input-6-eb896e587088> in <module>()
          1 p = proportion\_treated(data)
    ----> 2 print(f"Proportion Treated: \{p\} \textasciitilde{} \{int(p*100)\}\%")
    

        /opt/conda/lib/python3.6/site-packages/pandas/core/series.py in wrapper(self)
        116             return converter(self.iloc[0])
        117         raise TypeError("cannot convert the series to "
    --> 118                         "\{0\}".format(str(converter)))
        119 
        120     return wrapper


        TypeError: cannot convert the series to <class 'int'>

    \end{Verbatim}

     \#\#\# Exercise 02

Next, we can get a preliminary sense of the results by computing the
empirical 5-year death probability for the treated arm versus the
control arm.

The probability of dying for patients who received the treatment is:

\[p_{\text{treatment, death}} = \frac{n_{\text{treatment,death}}}{n_{\text{treatment}}}\]

\begin{itemize}
\tightlist
\item
  \(n_{\text{treatment,death}}\) is the number of patients who received
  the treatment and died.
\item
  \(n_{\text{treatment}}\) is the number of patients who received
  treatment.
\end{itemize}

The probability of dying for patients in the control group (who did not
received treatment) is:

\[p_{\text{control, death}} = \frac{n_{\text{control,death}}}{n_{\text{control}}}\]
- \(n_{\text{control,death}}\) is the number of patients in the control
group (did not receive the treatment) who died. - \(n_{\text{control}}\)
is the number of patients in the control group (did not receive
treatment).

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}7}]:} \PY{c+c1}{\PYZsh{} UNQ\PYZus{}C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)}
        \PY{k}{def} \PY{n+nf}{event\PYZus{}rate}\PY{p}{(}\PY{n}{df}\PY{p}{)}\PY{p}{:}
            \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}}
        \PY{l+s+sd}{    Compute empirical rate of death within 5 years}
        \PY{l+s+sd}{    for treated and untreated groups.}
        
        \PY{l+s+sd}{    Args:}
        \PY{l+s+sd}{        df (dataframe): dataframe containing trial results. }
        \PY{l+s+sd}{                          \PYZsq{}TRTMT\PYZsq{} column is 1 if patient was treated, 0 otherwise. }
        \PY{l+s+sd}{                            \PYZsq{}outcome\PYZsq{} column is 1 if patient died within 5 years, 0 otherwise.}
        \PY{l+s+sd}{  }
        \PY{l+s+sd}{    Returns:}
        \PY{l+s+sd}{        treated\PYZus{}prob (float): empirical probability of death given treatment}
        \PY{l+s+sd}{        untreated\PYZus{}prob (float): empirical probability of death given control}
        \PY{l+s+sd}{    \PYZsq{}\PYZsq{}\PYZsq{}}
            
            \PY{n}{treated\PYZus{}prob} \PY{o}{=} \PY{l+m+mf}{0.0}
            \PY{n}{control\PYZus{}prob} \PY{o}{=} \PY{l+m+mf}{0.0}
                
            \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} START CODE HERE (REPLACE INSTANCES OF \PYZsq{}None\PYZsq{} with your code) \PYZsh{}\PYZsh{}\PYZsh{}}
        
            
            \PY{n}{treated\PYZus{}prob} \PY{o}{=} \PY{n}{df}\PY{p}{[}\PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{TRTMT}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{==} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{outcome}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{count}\PY{p}{(}\PY{p}{)}\PY{o}{/}\PY{n+nb}{len}\PY{p}{(}\PY{n}{df}\PY{p}{)}
            \PY{n}{control\PYZus{}prob} \PY{o}{=} \PY{n}{df}\PY{p}{[}\PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{outcome}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{TRTMT}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{count}\PY{p}{(}\PY{p}{)}\PY{o}{/}\PY{n}{df}\PY{p}{[}\PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{TRTMT}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{outcome}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{count}\PY{p}{(}\PY{p}{)}
            
            \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} END CODE HERE \PYZsh{}\PYZsh{}\PYZsh{}}
        
            \PY{k}{return} \PY{n}{treated\PYZus{}prob}\PY{p}{,} \PY{n}{control\PYZus{}prob}
\end{Verbatim}


    \textbf{Test Case}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}8}]:} \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{TEST CASE}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{dataframe:}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n}{example\PYZus{}df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{data} \PY{o}{=}\PY{p}{[}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}
                                         \PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} 
                                         \PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}
                                         \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}
                                         \PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}
                                         \PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}
                                         \PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}
                                         \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{]}\PY{p}{,} \PY{n}{columns} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{outcome}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{TRTMT}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
        \PY{c+c1}{\PYZsh{}print(\PYZdq{}dataframe:\PYZbs{}n\PYZdq{})}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{example\PYZus{}df}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n}{treated\PYZus{}prob}\PY{p}{,} \PY{n}{control\PYZus{}prob} \PY{o}{=} \PY{n}{event\PYZus{}rate}\PY{p}{(}\PY{n}{example\PYZus{}df}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Treated 5\PYZhy{}year death rate, expected: 0.5, got: }\PY{l+s+si}{\PYZob{}treated\PYZus{}prob:.4f\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Control 5\PYZhy{}year death rate, expected: 0.75, got: }\PY{l+s+si}{\PYZob{}control\PYZus{}prob:.4f\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
TEST CASE
dataframe:

   outcome  TRTMT
0        0      1
1        1      1
2        1      1
3        0      1
4        1      0
5        1      0
6        1      0
7        0      0


Treated 5-year death rate, expected: 0.5, got: 0.5000
Control 5-year death rate, expected: 0.75, got: 0.7500

    \end{Verbatim}

    Now let's try the function on the real data.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}9}]:} \PY{n}{treated\PYZus{}prob}\PY{p}{,} \PY{n}{control\PYZus{}prob} \PY{o}{=} \PY{n}{event\PYZus{}rate}\PY{p}{(}\PY{n}{data}\PY{p}{)}
        
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Death rate for treated patients: }\PY{l+s+si}{\PYZob{}treated\PYZus{}prob:.4f\PYZcb{}}\PY{l+s+s2}{ \PYZti{} }\PY{l+s+s2}{\PYZob{}}\PY{l+s+s2}{int(treated\PYZus{}prob*100)\PYZcb{}}\PY{l+s+s2}{\PYZpc{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Death rate for untreated patients: }\PY{l+s+si}{\PYZob{}control\PYZus{}prob:.4f\PYZcb{}}\PY{l+s+s2}{ \PYZti{} }\PY{l+s+s2}{\PYZob{}}\PY{l+s+s2}{int(control\PYZus{}prob*100)\PYZcb{}}\PY{l+s+s2}{\PYZpc{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Death rate for treated patients: 0.4909 \textasciitilde{} 49\%
Death rate for untreated patients: 1.1230 \textasciitilde{} 112\%

    \end{Verbatim}

    On average, it seemed like treatment had a positive effect.

\hypertarget{sanity-checks}{%
\paragraph{Sanity checks}\label{sanity-checks}}

It's important to compute these basic summary statistics as a sanity
check for more complex models later on. If they strongly disagree with
these robust summaries and there isn't a good reason, then there might
be a bug.

    \hypertarget{train-test-split}{%
\subsubsection{Train test split}\label{train-test-split}}

We'll now try to quantify the impact more precisely using statistical
models. Before we get started fitting models to analyze the data, let's
split it using the \texttt{train\_test\_split} function from
\texttt{sklearn}. While a hold-out test set isn't required for logistic
regression, it will be useful for comparing its performance to the ML
models later on.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}10}]:} \PY{c+c1}{\PYZsh{} As usual, split into dev and test set}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k}{import} \PY{n}{train\PYZus{}test\PYZus{}split}
         \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{l+m+mi}{18}\PY{p}{)}
         \PY{n}{random}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}
         
         \PY{n}{data} \PY{o}{=} \PY{n}{data}\PY{o}{.}\PY{n}{dropna}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
         \PY{n}{y} \PY{o}{=} \PY{n}{data}\PY{o}{.}\PY{n}{outcome}
         \PY{c+c1}{\PYZsh{} notice we are dropping a column here. Now our total columns will be 1 less than before}
         \PY{n}{X} \PY{o}{=} \PY{n}{data}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{outcome}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)} 
         \PY{n}{X\PYZus{}dev}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}dev}\PY{p}{,} \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{test\PYZus{}size} \PY{o}{=} \PY{l+m+mf}{0.25}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}11}]:} \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{dev set shape: }\PY{l+s+si}{\PYZob{}X\PYZus{}dev.shape\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{test set shape: }\PY{l+s+si}{\PYZob{}X\PYZus{}test.shape\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
dev set shape: (455, 13)
test set shape: (152, 13)

    \end{Verbatim}

     \#\# 2 Modeling Treatment Effect

     \#\#\# 2.1 Constant Treatment Effect

First, we will model the treatment effect using a standard logistic
regression. If \(x^{(i)}\) is the input vector, then this models the
probability of death within 5 years as
\[\sigma(\theta^T x^{(i)}) = \frac{1}{1 + exp(-\theta^T x^{(i)})},\]

where \$ \theta\^{}T x\^{}\{(i)\} = \sum\_\{j\} \theta\_j
x\^{}\{(i)\}\_j\$ is an inner product.

    For example, if we have three features, \(TRTMT\), \(AGE\), and \(SEX\),
then our probability of death would be written as:

\[\sigma(\theta^T x^{(i)}) = \frac{1}{1 + exp(-\theta_{TRTMT} x^{(i)}_{TRTMT} - \theta_{AGE}x_{AGE}^{(i)} - \theta_{SEX}x^{(i)}_{SEX})}.\]

    Another way to look at logistic regresion is as a linear model for the
``logit'' function, or ``log odds'':

\[logit(p) = \log \left(\frac{p}{1-p} \right)= \theta^T x^{(i)}\]

\begin{itemize}
\item
  ``Odds'' is defined as the probability of an event divided by the
  probability of not having the event: \(\frac{p}{1-p}\).
\item
  ``Log odds'', or ``logit'' function, is the natural log of the odds:
  \(log \left(\frac{p}{1-p} \right)\)
\end{itemize}

    In this example, \(x^{(i)}_{TRTMT}\) is the treatment variable.
Therefore, \(\theta_{TRTMT}\) tells you what the effect of treatment is.
If \(\theta_{TRTMT}\) is negative, then having treatment reduces the
log-odds of death, which means death is less likely than if you did not
have treatment.

Note that this assumes a constant relative treatment effect, since the
impact of treatment does not depend on any other covariates.

Typically, a randomized control trial (RCT) will seek to establish a
negative \(\theta_{TRTMT}\) (because the treatment is intended to reduce
risk of death), which corresponds to an odds ratio of less than 1.

An odds ratio of less than one implies the probability of death is less
than the probability of surviving.

\[ \frac{p}{1-p} < 1 \rightarrow p < 1-p\]

    Run the next cell to fit your logistic regression model.

You can use the entire dev set (and do not need to reserve a separate
validation set) because there is no need for hyperparameter tuning using
a validation set.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}12}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{linear\PYZus{}model} \PY{k}{import} \PY{n}{LogisticRegression}
         
         \PY{n}{lr} \PY{o}{=} \PY{n}{LogisticRegression}\PY{p}{(}\PY{n}{penalty}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{l2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{solver}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{lbfgs}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{max\PYZus{}iter}\PY{o}{=}\PY{l+m+mi}{10000}\PY{p}{)}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}dev}\PY{p}{,} \PY{n}{y\PYZus{}dev}\PY{p}{)}
\end{Verbatim}


    \hypertarget{calculating-the-odds-ratio}{%
\subsubsection{Calculating the Odds
ratio}\label{calculating-the-odds-ratio}}

You are interested in finding the odds for treatment relative to the
odds for the baseline.

\[ OddsRatio = \frac{Odds_{treatment}}{Odds_{baseline}}\]

where \[Odds_{treatment} = \frac{p_{treatment}}{1-p_{treatment}}\]

and

\[Odds_{baseline} = \frac{p_{baseline}}{1-p_{baseline}}\]

    If you look at the expression

\[\log \left(\frac{p}{1-p} \right)= \theta^T x^{(i)} = \theta_{treatment} \times x_{treatment}^{(i)} + \theta_{age} \times x_{age}^{(i)} + \cdots\]

Let's just let ``\(\theta \times x_{age}^{(i)} + \cdots\)'' stand for
all the other thetas and feature variables except for the treatment
\(\theta_{treatment}^{(i)}\), and \(x_{treatment}^{(i)}\) .

    \hypertarget{treatment}{%
\paragraph{Treatment}\label{treatment}}

To denote that the patient received treatment, we set
\(x_{treatment}^{(i)} = 1\). Which means the log odds for a treated
patient are:

\[ log( Odds_{treatment}) = \log \left(\frac{p_{treatment}}{1-p_{treatment}} \right) = \theta_{treatment} \times 1 + \theta_{age} \times x_{age}^{(i)} + \cdots\]

To get odds from log odds, use exponentiation (raise to the power of e)
to take the inverse of the natural log.

\[Odds_{treatment} = e^{log( Odds_{treatment})} = \left(\frac{p_{treatment}}{1-p_{treatment}} \right) = e^{\theta_{treatment} \times 1 + \theta_{age} \times x_{age}^{(i)} + \cdots}\]

    \hypertarget{control-baseline}{%
\paragraph{Control (baseline)}\label{control-baseline}}

Similarly, when the patient has no treatment, this is denoted by
\(x_{treatment}^{(i)} = 0\). So the log odds for the untreated patient
is:

\[log(Odds_{baseline}) = \log \left(\frac{p_{baseline}}{1-p_{baseline}} \right) = \theta_{treatment} \times 0 + \theta_{age} \times x_{age}^{(i)} + \cdots\]

\[ = 0 + \theta_{age} \times x_{age}^{(i)} + \cdots\]

To get odds from log odds, use exponentiation (raise to the power of e)
to take the inverse of the natural log.

\[Odds_{baseline} = e^{log(Odds_{baseline})} = \left(\frac{p_{baseline}}{1-p_{baseline}} \right) = e^{0 + \theta_{age} \times x_{age}^{(i)} + \cdots}\]

    \hypertarget{odds-ratio}{%
\paragraph{Odds Ratio}\label{odds-ratio}}

The Odds ratio is:

\[ OddsRatio = \frac{Odds_{treatment}}{Odds_{baseline}}\]

Doing some substitution:

\[ OddsRatio = \frac{e^{\theta_{treatment} \times 1 + \theta_{age} \times x_{age}^{(i)} + \cdots}}{e^{0 + \theta_{age} \times x_{age}^{(i)} + \cdots}}\]

Notice that \(e^{\theta_{age} \times x_{age}^{(i)} + \cdots}\) cancels
on top and bottom, so that:

\[ OddsRatio = \frac{e^{\theta_{treatment} \times 1}}{e^{0}}\]

Since \(e^{0} = 1\), This simplifies to:

\[ OddsRatio = e^{\theta_{treatment}}\]

     \#\#\# Exercise 03: Extract the treatment effect

Complete the \texttt{extract\_treatment\_effect} function to extract
\(\theta_{treatment}\) and then calculate the odds ratio of treatment
from the logistic regression model.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}13}]:} \PY{c+c1}{\PYZsh{} UNQ\PYZus{}C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)}
         \PY{k}{def} \PY{n+nf}{extract\PYZus{}treatment\PYZus{}effect}\PY{p}{(}\PY{n}{lr}\PY{p}{,} \PY{n}{data}\PY{p}{)}\PY{p}{:}
             \PY{n}{theta\PYZus{}TRTMT} \PY{o}{=} \PY{l+m+mf}{0.0}
             \PY{n}{TRTMT\PYZus{}OR} \PY{o}{=} \PY{l+m+mf}{0.0}
             \PY{n}{coeffs} \PY{o}{=} \PY{p}{\PYZob{}}\PY{n}{data}\PY{o}{.}\PY{n}{columns}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{:}\PY{n}{lr}\PY{o}{.}\PY{n}{coef\PYZus{}}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{data}\PY{o}{.}\PY{n}{columns}\PY{p}{)}\PY{p}{)}\PY{p}{\PYZcb{}}
             
             \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} START CODE HERE (REPLACE INSTANCES OF \PYZsq{}None\PYZsq{} with your code) \PYZsh{}\PYZsh{}\PYZsh{}}
             
             \PY{c+c1}{\PYZsh{} get the treatment coefficient}
             \PY{n}{theta\PYZus{}TRTMT} \PY{o}{=} \PY{n}{coeffs}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{TRTMT}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
             
             \PY{c+c1}{\PYZsh{} calculate the Odds ratio for treatment}
             \PY{n}{TRTMT\PYZus{}OR} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{n}{theta\PYZus{}TRTMT}\PY{p}{)}
             
             \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} END CODE HERE \PYZsh{}\PYZsh{}\PYZsh{}}
             \PY{k}{return} \PY{n}{theta\PYZus{}TRTMT}\PY{p}{,} \PY{n}{TRTMT\PYZus{}OR}
\end{Verbatim}


    \hypertarget{test}{%
\paragraph{Test}\label{test}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}14}]:} \PY{c+c1}{\PYZsh{} Test extract\PYZus{}treatment\PYZus{}effect function}
         \PY{n}{theta\PYZus{}TRTMT}\PY{p}{,} \PY{n}{trtmt\PYZus{}OR} \PY{o}{=} \PY{n}{extract\PYZus{}treatment\PYZus{}effect}\PY{p}{(}\PY{n}{lr}\PY{p}{,} \PY{n}{X\PYZus{}dev}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Theta\PYZus{}TRTMT: }\PY{l+s+si}{\PYZob{}theta\PYZus{}TRTMT:.4f\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Treatment Odds Ratio: }\PY{l+s+si}{\PYZob{}trtmt\PYZus{}OR:.4f\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Theta\_TRTMT: -0.2885
Treatment Odds Ratio: 0.7494

    \end{Verbatim}

    \hypertarget{expected-output}{%
\subsubsection{Expected Output}\label{expected-output}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Theta_TRTMT: }\FloatTok{-0.2885}
\NormalTok{Treatment Odds Ratio: }\FloatTok{0.7494}
\end{Highlighting}
\end{Shaded}

    Based on this model, it seems that the treatment has a beneficial
effect.\\
- The \(\theta_{treatment} = -0.29\) is a negative value, meaning that
it has the effect of reducing risk of death. - In the code above, the
\(OddsRatio\) is stored in the variable \texttt{TRTMT\_OR}. - The
\(OddsRatio = 0.75\), which is less than 1.

You can think of the \(OddsRatio\) as a factor that is multiplied to the
baseline odds \(Odds_{baseline}\) in order to estimate the
\(Odds_{treatment}\). You can think about the Odds Ratio as a rate,
converting between baseline odds and treatment odds.

\[Odds_{treatment} = OddsRatio \times Odds_{baseline}\]

In this case:

\[Odds_{treatment} = 0.75 \times Odds_{baseline}\]

So you can interpret this to mean that the treatment reduces the odds of
death by \((1 - OddsRatio) = 1 - 0.75 = 0.25\), or about 25\%.

You will see how well this model fits the data in the next few sections.

     \#\#\# 2.2 Absolute Risk Reduction

     \#\#\# Exercise 4: Calculate ARR

A valuable quantity is the absolute risk reduction (ARR) of a treatment.
If \(p\) is the baseline probability of death, and \(p_{treatment}\) is
the probability of death if treated, then
\[ARR = p_{baseline} - p_{treatment} \]

In the case of logistic regression, here is how ARR can be computed:\\
Recall that the Odds Ratio is defined as:

\[OR = Odds_{treatment} / Odds_{baseline}\]

where the ``odds'' is the probability of the event over the probability
of not having the event, or \(p/(1-p)\).

\[Odds_{trtmt} = \frac{p_{treatment}}{1- p_{treatment}}\] and
\[Odds_{baseline} = \frac{p_{baseline}}{1- p_{baseline}}\]

In the function below, compute the predicted absolute risk reduction
(ARR) given - the odds ratio for treatment ``\(OR\)'', and - the
baseline risk of an individual \(p_{baseline}\)

If you get stuck, try reviewing the level 1 hints by clicking on the
cell ``Hints Level 1''. If you would like more help, please try viewing
``Hints Level 2''.

     Hints Level 1

Using the given \(p\), compute the baseline odds of death.

Then, use the Odds Ratio to convert that to odds of death given
treatment.

Finally, convert those odds back into a probability

     Hints Level 2

Solve for p\_treatment starting with this expression: Odds\_treatment =
p\_treatment / (1 - p\_treatment). You may want to do this on a piece of
paper.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}15}]:} \PY{c+c1}{\PYZsh{} UNQ\PYZus{}C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)}
         \PY{k}{def} \PY{n+nf}{OR\PYZus{}to\PYZus{}ARR}\PY{p}{(}\PY{n}{p}\PY{p}{,} \PY{n}{OR}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{    Compute ARR for treatment for individuals given}
         \PY{l+s+sd}{    baseline risk and odds ratio of treatment.}
         
         \PY{l+s+sd}{    Args:}
         \PY{l+s+sd}{        p (float): baseline probability of risk (without treatment)}
         \PY{l+s+sd}{        OR (float): odds ratio of treatment versus baseline}
         
         \PY{l+s+sd}{    Returns:}
         \PY{l+s+sd}{        ARR (float): absolute risk reduction for treatment }
         \PY{l+s+sd}{      \PYZdq{}\PYZdq{}\PYZdq{}}
             
             \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} START CODE HERE (REPLACE INSTANCES OF \PYZsq{}None\PYZsq{} with your code) \PYZsh{}\PYZsh{}\PYZsh{}}
         
             \PY{c+c1}{\PYZsh{} compute baseline odds from p}
             \PY{n}{odds\PYZus{}baseline} \PY{o}{=} \PY{n}{p}\PY{o}{/}\PY{p}{(}\PY{l+m+mi}{1}\PY{o}{\PYZhy{}}\PY{n}{p}\PY{p}{)}
         
             \PY{c+c1}{\PYZsh{} compute odds of treatment using odds ratio}
             \PY{n}{odds\PYZus{}trtmt} \PY{o}{=} \PY{n}{OR} \PY{o}{*} \PY{n}{odds\PYZus{}baseline}
         
             \PY{c+c1}{\PYZsh{} compute new probability of death from treatment odds}
             \PY{n}{p\PYZus{}trtmt} \PY{o}{=} \PY{n}{odds\PYZus{}trtmt} \PY{o}{/} \PY{p}{(}\PY{n}{odds\PYZus{}trtmt} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{)}
         
             \PY{c+c1}{\PYZsh{} compute ARR using treated probability and baseline probability }
             \PY{n}{ARR} \PY{o}{=} \PY{n}{p} \PY{o}{\PYZhy{}} \PY{n}{p\PYZus{}trtmt}
             
             \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} END CODE HERE \PYZsh{}\PYZsh{}\PYZsh{}}
             
             \PY{k}{return} \PY{n}{ARR}
\end{Verbatim}


    \textbf{Test Case}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}16}]:} \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{TEST CASES}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{test\PYZus{}p}\PY{p}{,} \PY{n}{test\PYZus{}OR} \PY{o}{=} \PY{p}{(}\PY{l+m+mf}{0.75}\PY{p}{,} \PY{l+m+mf}{0.5}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{baseline p: }\PY{l+s+si}{\PYZob{}test\PYZus{}p\PYZcb{}}\PY{l+s+s2}{, OR: }\PY{l+s+si}{\PYZob{}test\PYZus{}OR\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Output: }\PY{l+s+s2}{\PYZob{}}\PY{l+s+s2}{OR\PYZus{}to\PYZus{}ARR(test\PYZus{}p, test\PYZus{}OR):.4f\PYZcb{}, Expected: }\PY{l+s+si}{\PYZob{}0.15\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         
         \PY{n}{test\PYZus{}p}\PY{p}{,} \PY{n}{test\PYZus{}OR} \PY{o}{=} \PY{p}{(}\PY{l+m+mf}{0.04}\PY{p}{,} \PY{l+m+mf}{1.2}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{baseline p: }\PY{l+s+si}{\PYZob{}test\PYZus{}p\PYZcb{}}\PY{l+s+s2}{, OR: }\PY{l+s+si}{\PYZob{}test\PYZus{}OR\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Output: }\PY{l+s+s2}{\PYZob{}}\PY{l+s+s2}{OR\PYZus{}to\PYZus{}ARR(test\PYZus{}p, test\PYZus{}OR):.4f\PYZcb{}, Expected: }\PY{l+s+s2}{\PYZob{}}\PY{l+s+s2}{\PYZhy{}0.0076\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
TEST CASES
baseline p: 0.75, OR: 0.5
Output: 0.1500, Expected: 0.15

baseline p: 0.04, OR: 1.2
Output: -0.0076, Expected: -0.0076

    \end{Verbatim}

    \hypertarget{visualize-the-treatment-effect-as-baseline-risk-varies}{%
\paragraph{Visualize the treatment effect as baseline risk
varies}\label{visualize-the-treatment-effect-as-baseline-risk-varies}}

The logistic regression model assumes that treatment has a constant
effect in terms of odds ratio and is independent of other covariates.

However, this does not mean that absolute risk reduction is necessarily
constant for any baseline risk \(\hat{p}\). To illustrate this, we can
plot absolute risk reduction as a function of baseline predicted risk
\(\hat{p}\).

Run the next cell to see the relationship between ARR and baseline risk
for the logistic regression model.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}17}]:} \PY{n}{ps} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mf}{0.001}\PY{p}{,} \PY{l+m+mf}{0.999}\PY{p}{,} \PY{l+m+mf}{0.001}\PY{p}{)}
         \PY{n}{diffs} \PY{o}{=} \PY{p}{[}\PY{n}{OR\PYZus{}to\PYZus{}ARR}\PY{p}{(}\PY{n}{p}\PY{p}{,} \PY{n}{trtmt\PYZus{}OR}\PY{p}{)} \PY{k}{for} \PY{n}{p} \PY{o+ow}{in} \PY{n}{ps}\PY{p}{]}
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{ps}\PY{p}{,} \PY{n}{diffs}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Absolute Risk Reduction for Constant Treatment OR}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Baseline Risk}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Absolute Risk Reduction}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_53_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Note that when viewed on an absolute scale, the treatment effect is not
constant, despite the fact that you used a model with no interactions
between the features (we didn't multiply two features together).

As shown in the plot, when the baseline risk is either very low (close
to zero) or very high (close to one), the Absolute Risk Reduction from
treatment is fairly low. When the baseline risk is closer to 0.5 the ARR
of treatment is higher (closer to 0.10).

It is always important to remember that baseline risk has a natural
effect on absolute risk reduction.

     \#\#\# 2.3 Model Limitations

We can now plot how closely the empirical (actual) risk reduction
matches the risk reduction that is predicted by the logistic regression
model.

This is complicated by the fact that for each patient, we only observe
one outcome (treatment or no treatment). - We can't give a patient
treatment, then go back in time and measure an alternative scenario
where the same patient did not receive the treatment. - Therefore, we
will group patients into groups based on their baseline risk as
predicted by the model, and then plot their empirical ARR within groups
that have similar baseline risks. - The empirical ARR is the death rate
of the untreated patients in that group minus the death rate of the
treated patients in that group.

\[ARR_{empirical} = p_{baseline} - p_{treatment}\]

     \#\#\# Exercise 5: Baseline Risk In the next cell, write a function to
compute the baseline risk of each patient using the logistic regression
model.

The baseline risk is the model's predicted probability that the patient
is predicted to die if they do not receive treatment.

You will later use the baseline risk of each patient to organize
patients into risk groups (that have similar baseline risks). This will
allow you to calculate the ARR within each risk group.

\[p_{baseline} = logisticRegression(Treatment = False, Age = age_{i}, Obstruct = obstruct_{i}, \cdots)\]

     Hints

A patient receives treatment if their feature x\_treatment is True, and
does not receive treatment when their x\_treatment is False.

For a patient who actually did receive treatment, you can ask the model
to predict their risk without receiving treatment by setting the
patient's x\_treatment to False.

The logistic regression predict\_proba() function returns a 2D array,
one row for each patient, and one column for each possible outcome (each
class). In this case, the two outcomes are either no death (0), or death
(1). To find out which column contains the probability for death, check
the order of the classes by using lr.classes\_

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}18}]:} \PY{c+c1}{\PYZsh{} UNQ\PYZus{}C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)}
         \PY{k}{def} \PY{n+nf}{base\PYZus{}risks}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{lr\PYZus{}model}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{    Compute baseline risks for each individual in X.}
         
         \PY{l+s+sd}{    Args:}
         \PY{l+s+sd}{        X (dataframe): data from trial. \PYZsq{}TRTMT\PYZsq{} column}
         \PY{l+s+sd}{                       is 1 if subject retrieved treatment, 0 otherwise}
         \PY{l+s+sd}{        lr\PYZus{}model (model): logistic regression model}
         \PY{l+s+sd}{    }
         \PY{l+s+sd}{    Returns:}
         \PY{l+s+sd}{        risks (np.array): array of predicted baseline risk}
         \PY{l+s+sd}{                          for each subject in X}
         \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
             
             \PY{c+c1}{\PYZsh{} first make a copy of the dataframe so as not to overwrite the original}
             \PY{n}{X} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{n}{deep}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
             
             \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} START CODE HERE (REPLACE INSTANCES OF \PYZsq{}None\PYZsq{} with your code) \PYZsh{}\PYZsh{}\PYZsh{}}
         
             \PY{c+c1}{\PYZsh{} Set the treatment variable to assume that the patient did not receive treatment}
             \PY{n}{X}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{TRTMT}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{k+kc}{False}
             \PY{c+c1}{\PYZsh{}y = X.outcome}
             \PY{c+c1}{\PYZsh{}X = X.drop(\PYZsq{}outcome\PYZsq{}, axis=1)}
             \PY{c+c1}{\PYZsh{}p\PYZus{}baseline = lr\PYZus{}model(treatment=False, )}
             
             \PY{c+c1}{\PYZsh{} Input the features into the model, and predict the probability of death.}
             \PY{n}{risks} \PY{o}{=} \PY{n}{lr\PYZus{}model}\PY{o}{.}\PY{n}{predict\PYZus{}proba}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{n}{lr\PYZus{}model}\PY{o}{.}\PY{n}{classes\PYZus{}}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{]}
             
             \PY{c+c1}{\PYZsh{} END CODE HERE}
         
             \PY{k}{return} \PY{n}{risks}
\end{Verbatim}


    \textbf{Test Case}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}19}]:} \PY{n}{example\PYZus{}df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{columns} \PY{o}{=} \PY{n}{X\PYZus{}dev}\PY{o}{.}\PY{n}{columns}\PY{p}{)}
         \PY{n}{example\PYZus{}df}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{p}{:}\PY{p}{]} \PY{o}{=} \PY{n}{X\PYZus{}dev}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{n}{X\PYZus{}dev}\PY{o}{.}\PY{n}{TRTMT} \PY{o}{==} \PY{l+m+mi}{1}\PY{p}{,} \PY{p}{:}\PY{p}{]}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{p}{:}\PY{p}{]}
         \PY{n}{example\PYZus{}df}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{p}{:}\PY{p}{]} \PY{o}{=} \PY{n}{example\PYZus{}df}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{p}{:}\PY{p}{]}
         \PY{n}{example\PYZus{}df}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{TRTMT}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{0}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{TEST CASE}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{example\PYZus{}df}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{example\PYZus{}df}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{TRTMT}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{]}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Base risks for both rows should be the same}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Baseline Risks: }\PY{l+s+s2}{\PYZob{}}\PY{l+s+s2}{base\PYZus{}risks(example\PYZus{}df.copy(deep=True), lr)\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
TEST CASE
  sex age obstruct perfor adhere nodes node4 TRTMT differ\_2.0 differ\_3.0  \textbackslash{}
0   1  60        0      0      0     3     0  True          1          0   
1   1  60        0      0      0     3     0     0          1          0   

  extent\_2 extent\_3 extent\_4  
0        0        1        0  
1        0        1        0  
  TRTMT
0  True
1     0


Base risks for both rows should be the same
Baseline Risks: [0.43115868 0.43115868]

    \end{Verbatim}

    \hypertarget{expected-output}{%
\paragraph{Expected output}\label{expected-output}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Base risks }\ControlFlowTok{for}\NormalTok{ both rows should be the same}
\NormalTok{Baseline Risks: [}\FloatTok{0.43115868} \FloatTok{0.43115868}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

     \#\#\# Exercise 6: ARR by quantile

Since the effect of treatment varies depending on the baseline risk, it
makes more sense to group patients who have similar baseline risks, and
then look at the outcomes of those who receive treatment versus those
who do not, to estimate the absolute risk reduction (ARR).

You'll now implement the \texttt{lr\_ARR\_quantile} function to plot
empirical average ARR for each quantile of base risk.

     Hints

Use pandas.cut to define intervals of bins of equal size. For example,
pd.cut(arr,5) uses the values in the list or array `arr' and returns the
intervals of 5 bins.

Use pandas.DataFrame.groupby to group by a selected column of the
dataframe. Then select the desired variable and apply an aggregator
function. For example, df.groupby(`col1'){[}`col2'{]}.sum() groups by
column 1, and then calculates the sum of column 2 for each group.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}20}]:} \PY{c+c1}{\PYZsh{} UNQ\PYZus{}C6 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)}
         \PY{k}{def} \PY{n+nf}{lr\PYZus{}ARR\PYZus{}quantile}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{lr}\PY{p}{)}\PY{p}{:}
             
             \PY{c+c1}{\PYZsh{} first make a deep copy of the features dataframe to calculate the base risks}
             \PY{n}{X} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{n}{deep}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
             
             \PY{c+c1}{\PYZsh{} Make another deep copy of the features dataframe to store baseline risk, risk\PYZus{}group, and y}
             \PY{n}{df} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{n}{deep}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
         
             \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} START CODE HERE (REPLACE INSTANCES OF \PYZsq{}None\PYZsq{} with your code) \PYZsh{}\PYZsh{}\PYZsh{}}
             \PY{c+c1}{\PYZsh{} Calculate the baseline risks (use the function that you just implemented)}
             \PY{n}{baseline\PYZus{}risk} \PY{o}{=} \PY{n}{base\PYZus{}risks}\PY{p}{(}\PY{n}{df}\PY{p}{,} \PY{n}{lr}\PY{p}{)}
             
             \PY{c+c1}{\PYZsh{} bin patients into 10 risk groups based on their baseline risks}
             \PY{n}{risk\PYZus{}groups} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{cut}\PY{p}{(}\PY{n}{baseline\PYZus{}risk}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)}
                 
             \PY{c+c1}{\PYZsh{} Store the baseline risk, risk\PYZus{}groups, and y into the new dataframe}
             \PY{n}{df}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{baseline\PYZus{}risk}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{baseline\PYZus{}risk}
             \PY{n}{df}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{risk\PYZus{}group}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{risk\PYZus{}groups}
             \PY{n}{df}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{y}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{y}
         
             \PY{c+c1}{\PYZsh{} select the subset of patients who did not actually receive treatment}
             \PY{n}{df\PYZus{}baseline} \PY{o}{=} \PY{n}{df}\PY{p}{[}\PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{TRTMT}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{==} \PY{k+kc}{False}\PY{p}{]}
             
             \PY{c+c1}{\PYZsh{} select the subset of patients who did actually receive treatment}
             \PY{n}{df\PYZus{}treatment} \PY{o}{=} \PY{n}{df}\PY{p}{[}\PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{TRTMT}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{==} \PY{k+kc}{True}\PY{p}{]}
             
             \PY{c+c1}{\PYZsh{} For baseline patients, group them by risk group, select their outcome \PYZsq{}y\PYZsq{}, and take the mean}
             \PY{n}{baseline\PYZus{}mean\PYZus{}by\PYZus{}risk\PYZus{}group} \PY{o}{=} \PY{n}{df\PYZus{}baseline}\PY{o}{.}\PY{n}{groupby}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{risk\PYZus{}group}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{y}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}
             
             \PY{c+c1}{\PYZsh{} For treatment patients, group them by risk group, select their outcome \PYZsq{}y\PYZsq{}, and take the mean}
             \PY{n}{treatment\PYZus{}mean\PYZus{}by\PYZus{}risk\PYZus{}group} \PY{o}{=} \PY{n}{df\PYZus{}treatment}\PY{o}{.}\PY{n}{groupby}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{risk\PYZus{}group}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{y}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}
             
             \PY{c+c1}{\PYZsh{} Calculate the absolute risk reduction by risk group (baseline minus treatment)}
             \PY{n}{arr\PYZus{}by\PYZus{}risk\PYZus{}group} \PY{o}{=} \PY{n}{baseline\PYZus{}mean\PYZus{}by\PYZus{}risk\PYZus{}group} \PY{o}{\PYZhy{}} \PY{n}{treatment\PYZus{}mean\PYZus{}by\PYZus{}risk\PYZus{}group}
             
             \PY{c+c1}{\PYZsh{} Set the index of the arr\PYZus{}by\PYZus{}risk\PYZus{}group dataframe to the average baseline risk of each risk group }
             \PY{c+c1}{\PYZsh{} Use data for all patients to calculate the average baseline risk, grouped by risk group.}
             \PY{n}{arr\PYZus{}by\PYZus{}risk\PYZus{}group}\PY{o}{.}\PY{n}{index} \PY{o}{=} \PY{n}{df}\PY{o}{.}\PY{n}{groupby}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{risk\PYZus{}group}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{baseline\PYZus{}risk}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}
         
             \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} END CODE HERE \PYZsh{}\PYZsh{}\PYZsh{}}
             
             \PY{c+c1}{\PYZsh{} Set the name of the Series to \PYZsq{}ARR\PYZsq{}}
             \PY{n}{arr\PYZus{}by\PYZus{}risk\PYZus{}group}\PY{o}{.}\PY{n}{name} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ARR}\PY{l+s+s1}{\PYZsq{}}
             
         
             \PY{k}{return} \PY{n}{arr\PYZus{}by\PYZus{}risk\PYZus{}group}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}21}]:} \PY{c+c1}{\PYZsh{} Test}
         \PY{n}{abs\PYZus{}risks} \PY{o}{=} \PY{n}{lr\PYZus{}ARR\PYZus{}quantile}\PY{p}{(}\PY{n}{X\PYZus{}dev}\PY{p}{,} \PY{n}{y\PYZus{}dev}\PY{p}{,} \PY{n}{lr}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} print the Series}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{abs\PYZus{}risks}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} just showing this as a Dataframe for easier viewing}
         \PY{n}{display}\PY{p}{(}\PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{abs\PYZus{}risks}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
baseline\_risk
0.231595    0.089744
0.314713    0.042857
0.386342   -0.014604
0.458883    0.122222
0.530568    0.142857
0.626937   -0.104072
0.693404    0.150000
0.777353    0.293706
0.836617    0.083333
0.918884    0.200000
Name: ARR, dtype: float64

    \end{Verbatim}

    
    \begin{verbatim}
                    ARR
baseline_risk          
0.231595       0.089744
0.314713       0.042857
0.386342      -0.014604
0.458883       0.122222
0.530568       0.142857
0.626937      -0.104072
0.693404       0.150000
0.777353       0.293706
0.836617       0.083333
0.918884       0.200000
    \end{verbatim}

    
    \hypertarget{expected-output}{%
\subparagraph{Expected output}\label{expected-output}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{baseline_risk}
\FloatTok{0.231595}    \FloatTok{0.089744}
\FloatTok{0.314713}    \FloatTok{0.042857}
\FloatTok{0.386342}   \FloatTok{-0.014604}
\FloatTok{0.458883}    \FloatTok{0.122222}
\FloatTok{0.530568}    \FloatTok{0.142857}
\FloatTok{0.626937}   \FloatTok{-0.104072}
\FloatTok{0.693404}    \FloatTok{0.150000}
\FloatTok{0.777353}    \FloatTok{0.293706}
\FloatTok{0.836617}    \FloatTok{0.083333}
\FloatTok{0.918884}    \FloatTok{0.200000}
\NormalTok{Name: ARR, dtype: float}\DecValTok{64}
\end{Highlighting}
\end{Shaded}

    Plot the ARR grouped by baseline risk

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}22}]:} \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{abs\PYZus{}risks}\PY{o}{.}\PY{n}{index}\PY{p}{,} \PY{n}{abs\PYZus{}risks}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{empirical ARR}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Empirical Absolute Risk Reduction vs. Baseline Risk}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Absolute Risk Reduction}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Baseline Risk Range}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{ps} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{n}{abs\PYZus{}risks}\PY{o}{.}\PY{n}{index}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{\PYZhy{}}\PY{l+m+mf}{0.05}\PY{p}{,} \PY{n}{abs\PYZus{}risks}\PY{o}{.}\PY{n}{index}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{+}\PY{l+m+mf}{0.05}\PY{p}{,} \PY{l+m+mf}{0.01}\PY{p}{)}
         \PY{n}{diffs} \PY{o}{=} \PY{p}{[}\PY{n}{OR\PYZus{}to\PYZus{}ARR}\PY{p}{(}\PY{n}{p}\PY{p}{,} \PY{n}{trtmt\PYZus{}OR}\PY{p}{)} \PY{k}{for} \PY{n}{p} \PY{o+ow}{in} \PY{n}{ps}\PY{p}{]}
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{ps}\PY{p}{,} \PY{n}{diffs}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{predicted ARR}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{loc}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{upper right}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_68_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    In the plot, the empirical absolute risk reduction is shown as circles,
whereas the predicted risk reduction from the logistic regression model
is given by the solid line.

If ARR depended only on baseline risk, then if we plotted actual
(empirical) ARR grouped by baseline risk, then it would follow the
model's predictions closely (the dots would be near the line in most
cases).

However, you can see that the empirical absolute risk reduction (shown
as circles) does not match the predicted risk reduction from the
logistic regression model (given by the solid line).

This may indicate that ARR may depend on more than simply the baseline
risk.

     \#\# 3 Evaluation Metric

     \#\#\# 3.1 C-statistic-for-benefit (C-for-benefit)

You'll now use a measure to evaluate the discriminative power of your
models for predicting ARR. Ideally, you could use something like the
regular Concordance index (also called C-statistic) from Course 2.
Proceeding by analogy, you'd like to estimate something like:

\[P(A \text{ has higher predicted ARR than } B| A \text{ experienced a greater risk reduction than } B).\]

    \hypertarget{the-ideal-data-cannot-be-observed}{%
\paragraph{The ideal data cannot be
observed}\label{the-ideal-data-cannot-be-observed}}

The fundamental problem is that for each person, you can only observe
either their treatment outcome or their baseline outcome. - The patient
either receives the treatment, or does not receive the treatment. You
can't go back in time to have the same patient undergo treatment and
then not have treatment. - This means that you can't determine what
their actual risk reduction was.

    \hypertarget{estimate-the-treateduntreated-patient-using-a-pair-of-patients}{%
\paragraph{Estimate the treated/untreated patient using a pair of
patients}\label{estimate-the-treateduntreated-patient-using-a-pair-of-patients}}

What you will do instead is match people across treatment and control
arms based on predicted ARR. - Now, in each pair, you'll observe both
outcomes, so you'll have an estimate of the true treatment effect. - In
the pair of patients (A,B), - Patient A receives the treatment - Patient
B does not receive the treatment. - Think of the pair of patients as a
substitute for the the ideal data that has the same exact patient in
both the treatment and control group.

    \hypertarget{the-c-for-benefit}{%
\paragraph{The C-for-benefit}\label{the-c-for-benefit}}

\[P(\text{$P_1$ has a predicted ARR greater than $P_2$} | \text{$P_1$ experiences greater risk reduction than $P_2$}),\]

\begin{itemize}
\tightlist
\item
  Pair 1 consists of two patients (A,B), where A receives treatment, B
  does not.
\item
  Pair 2 is another pair of two patients (A,B), where A receives
  treatment, B does not.
\end{itemize}

The risk reduction for each pair is: - 1 if the treated person A
survives and the untreated B person does not (treatment helps).\\
- -1 if the treated person A dies and the untreated person B doesn't
(treatment harms) - 0 otherwise (treatment has no effect, because both
patients in the pair live, or both die).

    \hypertarget{details-for-calculating-c-for-benefit}{%
\paragraph{Details for calculating
C-for-benefit}\label{details-for-calculating-c-for-benefit}}

The c-for-benefit gives you a way to evaluate the ability of models to
discriminate between patient profiles which are likely to experience
greater benefit from treatment. - If you are better able to predict how
likely a treatment can improve a patient's outcome, you can help the
doctor and patient make a more informed decision when deciding whether
to undergo treatment, considering the possible side-effects and other
risks associated with treatment.

Please complete the implementation of the C-statistic-for-benefit below.

The code to create the pairs is given to you.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{obs_benefit_dict = \{}
\NormalTok{        (}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{): }\DecValTok{0}\NormalTok{,}
\NormalTok{        (}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{): }\DecValTok{-1}\NormalTok{,}
\NormalTok{        (}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{): }\DecValTok{1}\NormalTok{,}
\NormalTok{        (}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{): }\DecValTok{0}\NormalTok{,}
\NormalTok{    \}}
\end{Highlighting}
\end{Shaded}

Here is the interpretation of this dictionary for a pair of patients,
(A,B), where A receives treatment and B does not:\\
- When patient A does not die, and neither does patient B,
\texttt{(0,\ 0)}, the observed benefit of treatment is 0. - When patient
A does not die, but patient B does die, \texttt{(0,\ 1)}, the observed
benefit is -1 (the treatment helped). - When patient A dies, but patient
B does not die, \texttt{(1,\ 0)}, the observed benefit is 1 (the
treatment was harmful) - When patient A dies and patient B dies,
\texttt{(0,\ 0)}, the observed benefit of treatment is 0.

Each patient in the pair is represented by a tuple \texttt{(ARR,\ y)}. -
Index 0 contains the predicted ARR, which is the predicted benefit from
treatment. - Index 1 contains the actual patient outcome: 0 for no
death, 1 for death.

So a pair of patients is represented as a tuple containing two tuples:

For example, Pair\_1 is
\texttt{(\ (ARR\_1\_A,\ y\_1\_A),(ARR\_1\_B,\ y\_1\_B))}, and the data
may look like: \texttt{(\ (0.60,\ 0),(0.40,\ 1))}.\\
- This means that patient A (who received treatment) has a predicted
benefit of 0.60 and does not die. - Patient B (who did not receive
treatment) has a predicted benefit of 0.40 and dies.

     \#\#\# Exercise 7: Calculate c for benefit score In
\texttt{c\_for\_benefit\_score}, you will compute the C-for-benefit
given the matched pairs.

\[\text{c for benefit score} = \frac{concordant + 0.5 \times risk\_ties}{permissible}\]

     Click here for Hints!

A pair of patients in this case are two patients whose data are used to
represent a single patient.

A pair of pairs is similar to what you think of as just a ``pair'' in
the course 2 concordance index. It's a pair of pairs of patients (four
patients total).

Each patient is represented by a tuple of two values. The first value is
the predicted risk reduction, and the second is the patient's outcome.

observed benefit: for each patient pair, the first patient is assumed to
be the one who received treatment, and second in the pair is the one who
did not receive treatment. Observed benefit is either 0 (no effect), -1
(treatment helped), 1 (treatment harmed)

predicted benefit: for each patient pair, take the mean of the two
predicted benefits. This is the first value in each patient's tuple.

permissible pair of pairs: observed benefit is different between the two
pairs of pairs of patients.

concordant pair: the observed benefit and predicted benefit of pair 1
are both less than those for pair 2; or, the observed and predicted
benefit of pair 1 are both greater than those for pair 2. Also, it
should be a permissible pair of pairs.

Risk tie: the predicted benefits of both pairs are equal, and it's also
a permissible pair of pairs.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}39}]:} \PY{c+c1}{\PYZsh{} UNQ\PYZus{}C7 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)}
         \PY{k}{def} \PY{n+nf}{c\PYZus{}for\PYZus{}benefit\PYZus{}score}\PY{p}{(}\PY{n}{pairs}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{    Compute c\PYZhy{}statistic\PYZhy{}for\PYZhy{}benefit given list of}
         \PY{l+s+sd}{    individuals matched across treatment and control arms. }
         
         \PY{l+s+sd}{    Args:}
         \PY{l+s+sd}{        pairs (list of tuples): each element of the list is a tuple of individuals,}
         \PY{l+s+sd}{                                the first from the control arm and the second from}
         \PY{l+s+sd}{                                the treatment arm. Each individual }
         \PY{l+s+sd}{                                p = (pred\PYZus{}outcome, actual\PYZus{}outcome) is a tuple of}
         \PY{l+s+sd}{                                their predicted outcome and actual outcome.}
         \PY{l+s+sd}{    Result:}
         \PY{l+s+sd}{        cstat (float): c\PYZhy{}statistic\PYZhy{}for\PYZhy{}benefit computed from pairs.}
         \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
             
             \PY{c+c1}{\PYZsh{} mapping pair outcomes to benefit}
             \PY{n}{obs\PYZus{}benefit\PYZus{}dict} \PY{o}{=} \PY{p}{\PYZob{}}
                 \PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{)}\PY{p}{:} \PY{l+m+mi}{0}\PY{p}{,}
                 \PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{:} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}
                 \PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{)}\PY{p}{:} \PY{l+m+mi}{1}\PY{p}{,}
                 \PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{:} \PY{l+m+mi}{0}\PY{p}{,}
             \PY{p}{\PYZcb{}}
             
             \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} START CODE HERE (REPLACE INSTANCES OF \PYZsq{}None\PYZsq{}, \PYZsq{}False\PYZsq{}, and \PYZsq{}pass\PYZsq{} with your code) \PYZsh{}\PYZsh{}\PYZsh{}}
         
             \PY{c+c1}{\PYZsh{} compute observed benefit for each pair}
             \PY{n}{obs\PYZus{}benefit} \PY{o}{=} \PY{p}{[}\PY{n}{obs\PYZus{}benefit\PYZus{}dict}\PY{p}{[}\PY{p}{(}\PY{n}{i}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{i}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{]} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{pairs}\PY{p}{]}
         
             \PY{c+c1}{\PYZsh{} compute average predicted benefit for each pair}
             \PY{n}{pred\PYZus{}benefit} \PY{o}{=} \PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{[}\PY{n}{i}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{i}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{]}\PY{p}{)} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{pairs}\PY{p}{]}
         
             \PY{n}{concordant\PYZus{}count}\PY{p}{,} \PY{n}{permissible\PYZus{}count}\PY{p}{,} \PY{n}{risk\PYZus{}tie\PYZus{}count} \PY{o}{=} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}
         
             \PY{c+c1}{\PYZsh{} iterate over pairs of pairs}
             \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{pairs}\PY{p}{)}\PY{p}{)}\PY{p}{:}
                 \PY{k}{for} \PY{n}{j} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{i} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{pairs}\PY{p}{)}\PY{p}{)}\PY{p}{:}
                     
                     \PY{c+c1}{\PYZsh{} if the observed benefit is different, increment permissible count}
                     \PY{k}{if} \PY{n}{obs\PYZus{}benefit}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{!=} \PY{n}{obs\PYZus{}benefit}\PY{p}{[}\PY{n}{j}\PY{p}{]}\PY{p}{:}
         
                         \PY{c+c1}{\PYZsh{} increment count of permissible pairs}
                         \PY{n}{permissible\PYZus{}count} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
                         
                         \PY{c+c1}{\PYZsh{} if concordant, increment count}
                         
                         \PY{k}{if} \PY{p}{(}\PY{n}{obs\PYZus{}benefit}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{\PYZlt{}} \PY{n}{obs\PYZus{}benefit}\PY{p}{[}\PY{n}{j}\PY{p}{]}\PY{p}{)} \PY{o}{==} \PY{p}{(}\PY{n}{pred\PYZus{}benefit}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{\PYZlt{}} \PY{n}{pred\PYZus{}benefit}\PY{p}{[}\PY{n}{j}\PY{p}{]}\PY{p}{)}\PY{p}{:} \PY{c+c1}{\PYZsh{} change to check for concordance}
                             \PY{n}{concordant\PYZus{}count} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
         
                         \PY{c+c1}{\PYZsh{} if risk tie, increment count}
                         \PY{k}{if} \PY{n}{pred\PYZus{}benefit}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{o}{==}\PY{n}{pred\PYZus{}benefit}\PY{p}{[}\PY{n}{j}\PY{p}{]}\PY{p}{:} \PY{c+c1}{\PYZsh{}change to check for risk ties}
                             \PY{n}{risk\PYZus{}tie\PYZus{}count} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
         
         
             \PY{c+c1}{\PYZsh{} compute c\PYZhy{}statistic\PYZhy{}for\PYZhy{}benefit}
             \PY{n}{cstat} \PY{o}{=} \PY{p}{(}\PY{n}{concordant\PYZus{}count} \PY{o}{+} \PY{l+m+mf}{0.5} \PY{o}{*} \PY{n}{risk\PYZus{}tie\PYZus{}count}\PY{p}{)}\PY{o}{/}\PY{n}{permissible\PYZus{}count}
             
             \PY{c+c1}{\PYZsh{} END CODE HERE}
             
             \PY{k}{return} \PY{n}{cstat}
\end{Verbatim}


    \textbf{Test Case}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}40}]:} \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{TEST CASE}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{tmp\PYZus{}pairs} \PY{o}{=} \PY{p}{[}\PY{p}{(}\PY{p}{(}\PY{l+m+mf}{0.64}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{l+m+mf}{0.54}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{)}\PY{p}{)}\PY{p}{,} 
                      \PY{p}{(}\PY{p}{(}\PY{l+m+mf}{0.44}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{)}\PY{p}{,}\PY{p}{(}\PY{l+m+mf}{0.40}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{p}{,} 
                      \PY{p}{(}\PY{p}{(}\PY{l+m+mf}{0.56}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{l+m+mf}{0.74}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{)}\PY{p}{)}\PY{p}{,} 
                      \PY{p}{(}\PY{p}{(}\PY{l+m+mf}{0.22}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{,}\PY{p}{(}\PY{l+m+mf}{0.22}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{p}{,} 
                      \PY{p}{(}\PY{p}{(}\PY{l+m+mf}{0.22}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,}\PY{p}{(}\PY{l+m+mf}{0.22}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{)}\PY{p}{]}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{pairs: }\PY{l+s+si}{\PYZob{}tmp\PYZus{}pairs\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{tmp\PYZus{}cstat} \PY{o}{=} \PY{n}{c\PYZus{}for\PYZus{}benefit\PYZus{}score}\PY{p}{(}\PY{n}{tmp\PYZus{}pairs}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Output: }\PY{l+s+si}{\PYZob{}tmp\PYZus{}cstat:.4f\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
TEST CASE
pairs: [((0.64, 1), (0.54, 0)), ((0.44, 0), (0.4, 1)), ((0.56, 1), (0.74, 0)), ((0.22, 0), (0.22, 1)), ((0.22, 1), (0.22, 0))]
Output: 0.7500

    \end{Verbatim}

    \hypertarget{expected-output}{%
\subparagraph{Expected Output}\label{expected-output}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{TEST CASE}
\NormalTok{pairs: [((}\FloatTok{0.64}\NormalTok{, }\DecValTok{1}\NormalTok{), (}\FloatTok{0.54}\NormalTok{, }\DecValTok{0}\NormalTok{)), ((}\FloatTok{0.44}\NormalTok{, }\DecValTok{0}\NormalTok{), (}\FloatTok{0.4}\NormalTok{, }\DecValTok{1}\NormalTok{)), ((}\FloatTok{0.56}\NormalTok{, }\DecValTok{1}\NormalTok{), (}\FloatTok{0.74}\NormalTok{, }\DecValTok{0}\NormalTok{)), ((}\FloatTok{0.22}\NormalTok{, }\DecValTok{0}\NormalTok{), (}\FloatTok{0.22}\NormalTok{, }\DecValTok{1}\NormalTok{)), ((}\FloatTok{0.22}\NormalTok{, }\DecValTok{1}\NormalTok{), (}\FloatTok{0.22}\NormalTok{, }\DecValTok{0}\NormalTok{))]}
\NormalTok{Output: }\FloatTok{0.7500}
\end{Highlighting}
\end{Shaded}

     \#\#\# Exercise 8: Create patient pairs and calculate c-for-benefit

You will implement the function \texttt{c\_statistic}, which prepares
the patient data and uses the c-for-benefit score function to calculate
the c-for-benefit:

\begin{itemize}
\item
  Take as input:

  \begin{itemize}
  \tightlist
  \item
    The predicted risk reduction \texttt{pred\_rr} (ARR)
  \item
    outcomes \texttt{y} (1 for death, 0 for no death)
  \item
    treatments \texttt{w} (1 for treatment, 0 for no treatment)
  \end{itemize}
\item
  Collect the predicted risk reduction, outcomes and treatments into
  tuples, one tuple for each patient.
\item
  Filter one list of tuples where patients did not receive treatment.
\item
  Filter another list of tuples where patients received treatment.
\item
  Make sure that there is one treated patient for each untreated
  patient.

  \begin{itemize}
  \tightlist
  \item
    If there are fewer treated patients, randomly sample a subset of
    untreated patients, one for each treated patient.
  \item
    If there are fewer untreated patients, randomly sample a subset of
    treated patients, one for each untreated patient.
  \end{itemize}
\item
  Sort treated patients by their predicted risk reduction, and similarly
  sort the untreated patients by predicted risk reduction.

  \begin{itemize}
  \tightlist
  \item
    This allows you to match the treated patient with the highest
    predicted risk reduction with the untreated patient with the highest
    predicted risk reduction. Similarly, the second highest treated
    patient is matched with the second highest untreated patient.
  \end{itemize}
\item
  Create pairs of treated and untreated patients.
\end{itemize}

     Hints

Use zip(a,b,c) to create tuples from two or more lists of equal length,
and use list(zip(a,b,c)) to store that as a list data type.

Use filter(lambda x: x{[}0{]} == True, some\_list) to filter a list
(such as a list of tuples) so that the 0th item in each tuple is equal
to True. Cast the result as a list using list(filter(lambda x: x{[}0{]}
== True, some\_list))

Use random.sample(some\_list, sub\_sample\_length) to sample a subset
from a list without replacement.

Use sorted(some\_list, key=lambda x: x{[}1{]}) to sort a list of tuples
by their value in index 1.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}53}]:} \PY{c+c1}{\PYZsh{} UNQ\PYZus{}C8 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)}
         \PY{k}{def} \PY{n+nf}{c\PYZus{}statistic}\PY{p}{(}\PY{n}{pred\PYZus{}rr}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{w}\PY{p}{,} \PY{n}{random\PYZus{}seed}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{    Return concordance\PYZhy{}for\PYZhy{}benefit, the proportion of all matched pairs with}
         \PY{l+s+sd}{    unequal observed benefit, in which the patient pair receiving greater}
         \PY{l+s+sd}{    treatment benefit was predicted to do so.}
         
         \PY{l+s+sd}{    Args: }
         \PY{l+s+sd}{        pred\PYZus{}rr (array): array of predicted risk reductions}
         \PY{l+s+sd}{        y (array): array of true outcomes}
         \PY{l+s+sd}{        w (array): array of true treatments }
         \PY{l+s+sd}{    }
         \PY{l+s+sd}{    Returns: }
         \PY{l+s+sd}{        cstat (float): calculated c\PYZhy{}stat\PYZhy{}for\PYZhy{}benefit}
         \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{k}{assert} \PY{n+nb}{len}\PY{p}{(}\PY{n}{pred\PYZus{}rr}\PY{p}{)} \PY{o}{==} \PY{n+nb}{len}\PY{p}{(}\PY{n}{w}\PY{p}{)} \PY{o}{==} \PY{n+nb}{len}\PY{p}{(}\PY{n}{y}\PY{p}{)}
             \PY{n}{random}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{n}{random\PYZus{}seed}\PY{p}{)}
             
             \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} START CODE HERE (REPLACE INSTANCES OF \PYZsq{}None\PYZsq{} with your code) \PYZsh{}\PYZsh{}\PYZsh{}}
             \PY{c+c1}{\PYZsh{} Collect pred\PYZus{}rr, y, and w into tuples for each patient}
             \PY{n}{tuples} \PY{o}{=} \PY{n+nb}{tuple}\PY{p}{(}\PY{n+nb}{zip}\PY{p}{(}\PY{n}{pred\PYZus{}rr}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{w}\PY{p}{)}\PY{p}{)}
             
             \PY{c+c1}{\PYZsh{} Collect untreated patient tuples, stored as a list}
             \PY{n}{untreated} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{n+nb}{filter}\PY{p}{(}\PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{n}{x}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{,} \PY{n}{tuples}\PY{p}{)}\PY{p}{)}
             
             \PY{c+c1}{\PYZsh{} Collect treated patient tuples, stored as a list}
             \PY{n}{treated} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{n+nb}{filter}\PY{p}{(}\PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{n}{x}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]} \PY{o}{==} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{tuples}\PY{p}{)}\PY{p}{)}
         
             \PY{c+c1}{\PYZsh{} randomly subsample to ensure every person is matched}
             
             \PY{c+c1}{\PYZsh{} if there are more untreated than treated patients,}
             \PY{c+c1}{\PYZsh{} randomly choose a subset of untreated patients, one for each treated patient.}
         
             \PY{k}{if} \PY{n+nb}{len}\PY{p}{(}\PY{n}{treated}\PY{p}{)} \PY{o}{\PYZlt{}} \PY{n+nb}{len}\PY{p}{(}\PY{n}{untreated}\PY{p}{)}\PY{p}{:}
                 \PY{n}{untreated} \PY{o}{=} \PY{n}{random}\PY{o}{.}\PY{n}{sample}\PY{p}{(}\PY{n}{untreated}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{treated}\PY{p}{)}\PY{p}{)}
                 
             \PY{c+c1}{\PYZsh{} if there are more treated than untreated patients,}
             \PY{c+c1}{\PYZsh{} randomly choose a subset of treated patients, one for each treated patient.}
             \PY{k}{if} \PY{n+nb}{len}\PY{p}{(}\PY{n}{untreated}\PY{p}{)} \PY{o}{\PYZlt{}} \PY{n+nb}{len}\PY{p}{(}\PY{n}{treated}\PY{p}{)}\PY{p}{:}
                 \PY{n}{treated} \PY{o}{=} \PY{n}{random}\PY{o}{.}\PY{n}{sample}\PY{p}{(}\PY{n}{treated}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{untreated}\PY{p}{)}\PY{p}{)}
                 
             \PY{k}{assert} \PY{n+nb}{len}\PY{p}{(}\PY{n}{untreated}\PY{p}{)} \PY{o}{==} \PY{n+nb}{len}\PY{p}{(}\PY{n}{treated}\PY{p}{)}
         
             \PY{c+c1}{\PYZsh{} Sort the untreated patients by their predicted risk reduction}
             \PY{n}{untreated} \PY{o}{=} \PY{n+nb}{sorted}\PY{p}{(}\PY{n}{untreated}\PY{p}{,} \PY{n}{key}\PY{o}{=}\PY{k}{lambda} \PY{n}{x} \PY{p}{:} \PY{n}{x}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
             
             \PY{c+c1}{\PYZsh{} Sort the treated patients by their predicted risk reduction}
             \PY{n}{treated} \PY{o}{=} \PY{n+nb}{sorted}\PY{p}{(}\PY{n}{treated}\PY{p}{,} \PY{n}{key}\PY{o}{=}\PY{k}{lambda} \PY{n}{x} \PY{p}{:} \PY{n}{x}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
             
             \PY{c+c1}{\PYZsh{} match untreated and treated patients to create pairs together}
             \PY{n}{pairs} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{n+nb}{zip}\PY{p}{(}\PY{n}{untreated}\PY{p}{,} \PY{n}{treated}\PY{p}{)}\PY{p}{)}
         
             \PY{c+c1}{\PYZsh{} calculate the c\PYZhy{}for\PYZhy{}benefit using these pairs (use the function that you implemented earlier)}
             \PY{n}{cstat} \PY{o}{=} \PY{n}{c\PYZus{}for\PYZus{}benefit\PYZus{}score}\PY{p}{(}\PY{n}{pairs}\PY{p}{)}
             
             \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} END CODE HERE \PYZsh{}\PYZsh{}\PYZsh{}}
             
             \PY{k}{return} \PY{n}{cstat}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}54}]:} \PY{c+c1}{\PYZsh{} Test}
         
         \PY{n}{tmp\PYZus{}pred\PYZus{}rr} \PY{o}{=} \PY{p}{[}\PY{l+m+mf}{0.1}\PY{p}{,}\PY{l+m+mf}{0.2}\PY{p}{,}\PY{l+m+mf}{0.3}\PY{p}{,}\PY{l+m+mf}{0.4}\PY{p}{,}\PY{l+m+mf}{0.5}\PY{p}{,}\PY{l+m+mf}{0.6}\PY{p}{,}\PY{l+m+mf}{0.7}\PY{p}{,}\PY{l+m+mf}{0.8}\PY{p}{,}\PY{l+m+mf}{0.9}\PY{p}{]}
         \PY{n}{tmp\PYZus{}y} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}
         \PY{n}{tmp\PYZus{}w} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}
         
         \PY{n}{tmp\PYZus{}cstat} \PY{o}{=} \PY{n}{c\PYZus{}statistic}\PY{p}{(}\PY{n}{tmp\PYZus{}pred\PYZus{}rr}\PY{p}{,} \PY{n}{tmp\PYZus{}y}\PY{p}{,} \PY{n}{tmp\PYZus{}w}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{C\PYZhy{}for\PYZhy{}benefit calculated is }\PY{l+s+si}{\PYZob{}tmp\PYZus{}cstat\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
C-for-benefit calculated is 0.6

    \end{Verbatim}

    \hypertarget{expected-output}{%
\subparagraph{Expected output}\label{expected-output}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{C-}\ControlFlowTok{for}\NormalTok{-benefit calculated is }\FloatTok{0.6}
\end{Highlighting}
\end{Shaded}

    \hypertarget{predicted-risk-reduction}{%
\subsubsection{Predicted risk
reduction}\label{predicted-risk-reduction}}

In order to compute the c-statistic-for-benefit for any of your models,
you need to compute predicted risk reduction from treatment (predicted
risk reduction is the input \texttt{pred\_rr} to the c-statistic
function).

\begin{itemize}
\tightlist
\item
  The easiest way to do this in general is to create a version of the
  data where the treatment variable is False and a version where it is
  True.
\item
  Then take the difference
  \(\text{pred_RR} = p_{control} - p_{treatment}\)
\end{itemize}

We've implemented this for you.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}55}]:} \PY{k}{def} \PY{n+nf}{treatment\PYZus{}control}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Create treatment and control versions of data\PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{n}{X\PYZus{}treatment} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{n}{deep}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
             \PY{n}{X\PYZus{}control} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{n}{deep}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
             \PY{n}{X\PYZus{}treatment}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{TRTMT}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{1}
             \PY{n}{X\PYZus{}control}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{TRTMT}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{0}
             \PY{k}{return} \PY{n}{X\PYZus{}treatment}\PY{p}{,} \PY{n}{X\PYZus{}control}
         
         \PY{k}{def} \PY{n+nf}{risk\PYZus{}reduction}\PY{p}{(}\PY{n}{model}\PY{p}{,} \PY{n}{data\PYZus{}treatment}\PY{p}{,} \PY{n}{data\PYZus{}control}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Compute predicted risk reduction for each row in data\PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{n}{treatment\PYZus{}risk} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{predict\PYZus{}proba}\PY{p}{(}\PY{n}{data\PYZus{}treatment}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}
             \PY{n}{control\PYZus{}risk} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{predict\PYZus{}proba}\PY{p}{(}\PY{n}{data\PYZus{}control}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}
             \PY{k}{return} \PY{n}{control\PYZus{}risk} \PY{o}{\PYZhy{}} \PY{n}{treatment\PYZus{}risk}
\end{Verbatim}


    Now let's compute the predicted risk reductions of the logistic
regression model on the test set.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}56}]:} \PY{n}{X\PYZus{}test\PYZus{}treated}\PY{p}{,} \PY{n}{X\PYZus{}test\PYZus{}untreated} \PY{o}{=} \PY{n}{treatment\PYZus{}control}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}
         \PY{n}{rr\PYZus{}lr} \PY{o}{=} \PY{n}{risk\PYZus{}reduction}\PY{p}{(}\PY{n}{lr}\PY{p}{,} \PY{n}{X\PYZus{}test\PYZus{}treated}\PY{p}{,} \PY{n}{X\PYZus{}test\PYZus{}untreated}\PY{p}{)}
\end{Verbatim}


    Before we evaluate the c-statistic-for-benefit, let's look at a
histogram of predicted ARR.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}57}]:} \PY{n}{plt}\PY{o}{.}\PY{n}{hist}\PY{p}{(}\PY{n}{rr\PYZus{}lr}\PY{p}{,} \PY{n}{bins}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{auto}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Histogram of Predicted ARR using logistic regression}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{count of patients}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ARR}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_92_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Note that although it predicts different absolute risk reduction, it
never predicts that the treatment will adversely impact risk. This is
because the odds ratio of treatment is less than 1, so the model always
predicts a decrease in the baseline risk. Run the next cell to compute
the c-statistic-for-benefit on the test data.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}58}]:} \PY{n}{tmp\PYZus{}cstat\PYZus{}test} \PY{o}{=} \PY{n}{c\PYZus{}statistic}\PY{p}{(}\PY{n}{rr\PYZus{}lr}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{o}{.}\PY{n}{TRTMT}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Logistic Regression evaluated by C\PYZhy{}for\PYZhy{}Benefit: }\PY{l+s+si}{\PYZob{}tmp\PYZus{}cstat\PYZus{}test:.4f\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Logistic Regression evaluated by C-for-Benefit: 0.5412

    \end{Verbatim}

    \hypertarget{expected-output}{%
\subparagraph{Expected Output}\label{expected-output}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Logistic Regression evaluated by C-}\ControlFlowTok{for}\NormalTok{-Benefit: }\FloatTok{0.5412}
\end{Highlighting}
\end{Shaded}

    Recall that a c statistic ranges from 0 to 1, and is closer to when the
model being evaluated is doing a good job with its predictions.

You can see that the model is not doing a great job of predicting risk
reduction, given a c-for-benefit of around 0.54.

    \hypertarget{regular-c-index}{%
\subsubsection{Regular c-index}\label{regular-c-index}}

Let's compare this with the regular C-index which you've applied in
previous assignments. Note that the regular c-statistic does not look at
pairs of pairs of patients, and just compares one patient to another
when evaluating the model's performance. So the regular c-index is
evaluating the model's ability to predict overall patient risk, not
necessarily measuring how well the model predicts benefit from
treatment.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}59}]:} \PY{k+kn}{from} \PY{n+nn}{lifelines}\PY{n+nn}{.}\PY{n+nn}{utils} \PY{k}{import} \PY{n}{concordance\PYZus{}index}
         \PY{n}{tmp\PYZus{}regular\PYZus{}cindex} \PY{o}{=} \PY{n}{concordance\PYZus{}index}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{lr}\PY{o}{.}\PY{n}{predict\PYZus{}proba}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Logistic Regression evaluated by regular C\PYZhy{}index: }\PY{l+s+si}{\PYZob{}tmp\PYZus{}regular\PYZus{}cindex:.4f\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Logistic Regression evaluated by regular C-index: 0.7785

    \end{Verbatim}

    \hypertarget{expected-output}{%
\subparagraph{Expected output}\label{expected-output}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Logistic Regression evaluated by regular C-index: }\FloatTok{0.7785}
\end{Highlighting}
\end{Shaded}

    You can see that even though the model accurately predicts overall risk
(regular c-index), it does not necessarily do a great job predicting
benefit from treatment (c-for-benefit).

    You can also visually assess the discriminative ability of the model by
checking if the people it thinks benefit the most from treatment
empirically (actually) experience a benefit.

Since you don't have counterfactual results from individuals, you'll
need to aggregate patient information in some way.

You can group patients by deciles (10 groups) of risk.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}60}]:} \PY{k}{def} \PY{n+nf}{quantile\PYZus{}benefit}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{arr\PYZus{}hat}\PY{p}{)}\PY{p}{:}
             \PY{n}{df} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{n}{deep}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
             \PY{n}{df}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{y}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{y}
             \PY{n}{df}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{benefit}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{arr\PYZus{}hat}
             \PY{n}{benefit\PYZus{}groups} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{qcut}\PY{p}{(}\PY{n}{arr\PYZus{}hat}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)}
             \PY{n}{df}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{benefit\PYZus{}groups}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{benefit\PYZus{}groups}
             \PY{n}{empirical\PYZus{}benefit} \PY{o}{=} \PY{n}{df}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{n}{df}\PY{o}{.}\PY{n}{TRTMT} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{,} \PY{p}{:}\PY{p}{]}\PY{o}{.}\PY{n}{groupby}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{benefit\PYZus{}groups}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{y}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{df}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{n}{df}\PY{o}{.}\PY{n}{TRTMT} \PY{o}{==} \PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{groupby}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{benefit\PYZus{}groups}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{y}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}
             \PY{n}{avg\PYZus{}benefit} \PY{o}{=} \PY{n}{df}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{n}{df}\PY{o}{.}\PY{n}{TRTMT} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{,} \PY{p}{:}\PY{p}{]}\PY{o}{.}\PY{n}{y}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{df}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{n}{df}\PY{o}{.}\PY{n}{TRTMT}\PY{o}{==}\PY{l+m+mi}{1}\PY{p}{,} \PY{p}{:}\PY{p}{]}\PY{o}{.}\PY{n}{y}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}
             \PY{k}{return} \PY{n}{empirical\PYZus{}benefit}\PY{p}{,} \PY{n}{avg\PYZus{}benefit}
         
         \PY{k}{def} \PY{n+nf}{plot\PYZus{}empirical\PYZus{}risk\PYZus{}reduction}\PY{p}{(}\PY{n}{emp\PYZus{}benefit}\PY{p}{,} \PY{n}{av\PYZus{}benefit}\PY{p}{,} \PY{n}{model}\PY{p}{)}\PY{p}{:}
             \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{emp\PYZus{}benefit}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{n}{emp\PYZus{}benefit}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{xticks}\PY{p}{(}\PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{emp\PYZus{}benefit}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{emp\PYZus{}benefit}\PY{p}{)} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Empirical Risk Reduction vs. Predicted (}\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{)}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{model}\PY{p}{)}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Empirical Risk Reduction}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Predicted Risk Reduction Quantile}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{)}\PY{p}{,} \PY{p}{[}\PY{n}{av\PYZus{}benefit}\PY{p}{]}\PY{o}{*}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{linestyle}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{average RR}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{loc}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{lower right}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
         
         \PY{n}{emp\PYZus{}benefit}\PY{p}{,} \PY{n}{avg\PYZus{}benefit} \PY{o}{=} \PY{n}{quantile\PYZus{}benefit}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{rr\PYZus{}lr}\PY{p}{)}
         \PY{n}{plot\PYZus{}empirical\PYZus{}risk\PYZus{}reduction}\PY{p}{(}\PY{n}{emp\PYZus{}benefit}\PY{p}{,} \PY{n}{avg\PYZus{}benefit}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Logistic Regression}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_102_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    If the model performed well, then you would see patients in the higher
deciles of predicted risk reduction (on the right) also have higher
empirical risk reduction (to the top).

This model using logistic regression is far from perfect.

Below, you'll see if you can do better using a more flexible machine
learning approach.

     \#\# 4 Machine Learning Approaches

     \#\#\# 4.1 T-Learner

Now you will see how recent machine learning approaches compare to the
more standard analysis. The approach we'll look at is called
\href{https://arxiv.org/pdf/1706.03461.pdf}{T-learner}. - ``T'' stands
for ``two''. - The T-learner learns two different models, one for
treatment risk, and another model for control risk. - Then takes the
difference of the two risk predictions to predict the risk reduction.

     \#\#\# Exercise 9: Complete the TLearner class.

\begin{itemize}
\tightlist
\item
  The constructor \texttt{\_\_init\_\_()} sets the treatment and control
  estimators based on the given inputs to the constructor.
\item
  The \texttt{predict} function takes the features and uses each
  estimator to predict the risk of death. Then it calculates the risk of
  death for the control estimator minus the risk of death from the
  treatment estimator, and returns this as the predicted risk reduction.
\end{itemize}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}62}]:} \PY{c+c1}{\PYZsh{} UNQ\PYZus{}C9 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)}
         \PY{k}{class} \PY{n+nc}{TLearner}\PY{p}{(}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{    T\PYZhy{}Learner class.}
         
         \PY{l+s+sd}{    Attributes:}
         \PY{l+s+sd}{      treatment\PYZus{}estimator (object): fitted model for treatment outcome}
         \PY{l+s+sd}{      control\PYZus{}estimator (object): fitted model for control outcome}
         \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}                               
             \PY{k}{def} \PY{n+nf}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{treatment\PYZus{}estimator}\PY{p}{,} \PY{n}{control\PYZus{}estimator}\PY{p}{)}\PY{p}{:}
                 \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{        Initializer for TLearner class.}
         \PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
                 \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} START CODE HERE (REPLACE INSTANCES OF \PYZsq{}None\PYZsq{} with your code) \PYZsh{}\PYZsh{}\PYZsh{}}
                 \PY{c+c1}{\PYZsh{} set the treatment estimator}
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{treatment\PYZus{}estimator} \PY{o}{=} \PY{n}{treatment\PYZus{}estimator}
                 
                 \PY{c+c1}{\PYZsh{} set the control estimator }
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{control\PYZus{}estimator} \PY{o}{=} \PY{n}{control\PYZus{}estimator}
                 
                 \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} END CODE HERE \PYZsh{}\PYZsh{}\PYZsh{}}
         
             \PY{k}{def} \PY{n+nf}{predict}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{X}\PY{p}{)}\PY{p}{:}
                 \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{        Return predicted risk reduction for treatment for given data matrix.}
         
         \PY{l+s+sd}{        Args:}
         \PY{l+s+sd}{          X (dataframe): dataframe containing features for each subject}
         \PY{l+s+sd}{    }
         \PY{l+s+sd}{        Returns:}
         \PY{l+s+sd}{          preds (np.array): predicted risk reduction for each row of X}
         \PY{l+s+sd}{        \PYZdq{}\PYZdq{}\PYZdq{}}
                 \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} START CODE HERE (REPLACE INSTANCES OF \PYZsq{}None\PYZsq{} with your code) \PYZsh{}\PYZsh{}\PYZsh{}}
                 \PY{c+c1}{\PYZsh{} predict the risk of death using the control estimator}
                 \PY{n}{risk\PYZus{}control} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{control\PYZus{}estimator}\PY{o}{.}\PY{n}{predict\PYZus{}proba}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}
                 
                 \PY{c+c1}{\PYZsh{} predict the risk of death using the treatment estimator}
                 \PY{n}{risk\PYZus{}treatment} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{treatment\PYZus{}estimator}\PY{o}{.}\PY{n}{predict\PYZus{}proba}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}
                 
                 \PY{c+c1}{\PYZsh{} the predicted risk reduction is control risk minus the treatment risk}
                 \PY{n}{pred\PYZus{}risk\PYZus{}reduction} \PY{o}{=}  \PY{n}{risk\PYZus{}control} \PY{o}{\PYZhy{}} \PY{n}{risk\PYZus{}treatment}
                 
                 \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} END CODE HERE \PYZsh{}\PYZsh{}\PYZsh{}}
                         
                 \PY{k}{return} \PY{n}{pred\PYZus{}risk\PYZus{}reduction}
\end{Verbatim}


    \hypertarget{tune-the-model-with-grid-search}{%
\subsubsection{Tune the model with grid
search}\label{tune-the-model-with-grid-search}}

In order to tune your two models, you will use grid search to find the
desired parameters. - You will use a validation set to evaluate the
model on different parameters, in order to avoid overfitting to the
training set.

To test models on all combinations of hyperparameters, you can first
list out all of the values in a list of lists. For example:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{hyperparams = \{}
    \CharTok{'n_estimators'}\NormalTok{: [}\DecValTok{10}\NormalTok{, }\DecValTok{20}\NormalTok{],}
    \CharTok{'max_depth'}\NormalTok{: [}\DecValTok{2}\NormalTok{, }\DecValTok{5}\NormalTok{],}
    \CharTok{'min_samples_leaf'}\NormalTok{: [}\FloatTok{0.1}\NormalTok{, }\FloatTok{0.2}\NormalTok{],}
    \CharTok{'random_state'}\NormalTok{: [}\DecValTok{0}\NormalTok{]}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

You can generate a list like this:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{[[}\AttributeTok{10}\NormalTok{, }\AttributeTok{20}\NormalTok{],}
\NormalTok{ [}\AttributeTok{2}\NormalTok{, }\AttributeTok{5}\NormalTok{], }
\NormalTok{ [}\AttributeTok{0}\NormalTok{.}\AttributeTok{1}\NormalTok{, }\AttributeTok{0}\NormalTok{.}\AttributeTok{2}\NormalTok{]}
\NormalTok{]}
\end{Highlighting}
\end{Shaded}

Next, you can get all combinations of the hyperparameter values:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{[(}\DecValTok{10}\NormalTok{, }\DecValTok{2}\NormalTok{, }\FloatTok{0.1}\NormalTok{),}
\NormalTok{ (}\DecValTok{10}\NormalTok{, }\DecValTok{2}\NormalTok{, }\FloatTok{0.2}\NormalTok{),}
\NormalTok{ (}\DecValTok{10}\NormalTok{, }\DecValTok{5}\NormalTok{, }\FloatTok{0.1}\NormalTok{),}
\NormalTok{ (}\DecValTok{10}\NormalTok{, }\DecValTok{5}\NormalTok{, }\FloatTok{0.2}\NormalTok{),}
\NormalTok{ (}\DecValTok{20}\NormalTok{, }\DecValTok{2}\NormalTok{, }\FloatTok{0.1}\NormalTok{),}
\NormalTok{ (}\DecValTok{20}\NormalTok{, }\DecValTok{2}\NormalTok{, }\FloatTok{0.2}\NormalTok{),}
\NormalTok{ (}\DecValTok{20}\NormalTok{, }\DecValTok{5}\NormalTok{, }\FloatTok{0.1}\NormalTok{),}
\NormalTok{ (}\DecValTok{20}\NormalTok{, }\DecValTok{5}\NormalTok{, }\FloatTok{0.2}\NormalTok{)]}
\end{Highlighting}
\end{Shaded}

To feed the hyperparameters into an random forest model, you can use a
dictionary, so that you do not need to hard code the parameter names.
For example, instead of

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{RandomForestClassifier(n_estimators= }\DecValTok{20}\NormalTok{, max_depth=}\DecValTok{5}\NormalTok{, min_samples_leaf=}\FloatTok{0.2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

You have more flexibility if you create a dictionary and pass it into
the model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{args_d = \{}\CharTok{'n_estimators'}\NormalTok{: }\DecValTok{20}\NormalTok{, }\CharTok{'max_depth'}\NormalTok{: }\DecValTok{5}\NormalTok{, }\CharTok{'min_samples_leaf'}\NormalTok{: }\FloatTok{0.2}\NormalTok{\}}
\NormalTok{RandomForestClassifier(**args_d)}
\end{Highlighting}
\end{Shaded}

This allows you to pass in a hyperparameter dictionary for any
hyperpameters, not just \texttt{n\_estimators}, \texttt{max\_depth}, and
\texttt{min\_samples\_leaf}.

So you'll find a way to generate a list of dictionaries, like this:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{[\{}\CharTok{'n_estimators'}\NormalTok{: }\DecValTok{10}\NormalTok{, }\CharTok{'max_depth'}\NormalTok{: }\DecValTok{2}\NormalTok{, }\CharTok{'min_samples_leaf'}\NormalTok{: }\FloatTok{0.1}\NormalTok{\},}
\NormalTok{ \{}\CharTok{'n_estimators'}\NormalTok{: }\DecValTok{10}\NormalTok{, }\CharTok{'max_depth'}\NormalTok{: }\DecValTok{2}\NormalTok{, }\CharTok{'min_samples_leaf'}\NormalTok{: }\FloatTok{0.2}\NormalTok{\},}
\NormalTok{ \{}\CharTok{'n_estimators'}\NormalTok{: }\DecValTok{10}\NormalTok{, }\CharTok{'max_depth'}\NormalTok{: }\DecValTok{5}\NormalTok{, }\CharTok{'min_samples_leaf'}\NormalTok{: }\FloatTok{0.1}\NormalTok{\},}
\NormalTok{ \{}\CharTok{'n_estimators'}\NormalTok{: }\DecValTok{10}\NormalTok{, }\CharTok{'max_depth'}\NormalTok{: }\DecValTok{5}\NormalTok{, }\CharTok{'min_samples_leaf'}\NormalTok{: }\FloatTok{0.2}\NormalTok{\},}
\NormalTok{ \{}\CharTok{'n_estimators'}\NormalTok{: }\DecValTok{20}\NormalTok{, }\CharTok{'max_depth'}\NormalTok{: }\DecValTok{2}\NormalTok{, }\CharTok{'min_samples_leaf'}\NormalTok{: }\FloatTok{0.1}\NormalTok{\},}
\NormalTok{ \{}\CharTok{'n_estimators'}\NormalTok{: }\DecValTok{20}\NormalTok{, }\CharTok{'max_depth'}\NormalTok{: }\DecValTok{2}\NormalTok{, }\CharTok{'min_samples_leaf'}\NormalTok{: }\FloatTok{0.2}\NormalTok{\},}
\NormalTok{ \{}\CharTok{'n_estimators'}\NormalTok{: }\DecValTok{20}\NormalTok{, }\CharTok{'max_depth'}\NormalTok{: }\DecValTok{5}\NormalTok{, }\CharTok{'min_samples_leaf'}\NormalTok{: }\FloatTok{0.1}\NormalTok{\},}
\NormalTok{ \{}\CharTok{'n_estimators'}\NormalTok{: }\DecValTok{20}\NormalTok{, }\CharTok{'max_depth'}\NormalTok{: }\DecValTok{5}\NormalTok{, }\CharTok{'min_samples_leaf'}\NormalTok{: }\FloatTok{0.2}\NormalTok{\}]}
\end{Highlighting}
\end{Shaded}

Notice how the values in both the list of tuples and list of
dictionaries are in the same order as the original hyperparams
dictionary. For example, the first value in each is n\_estimarors, then
max\_depth, and then min\_samples\_leaf:

\begin{Shaded}
\begin{Highlighting}[]
\ErrorTok{# list of lists}
\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{2}\NormalTok{, }\FloatTok{0.1}\NormalTok{)}

\ErrorTok{# list of dictionaries}
\NormalTok{\{}\CharTok{'n_estimators'}\NormalTok{: }\DecValTok{10}\NormalTok{, }\CharTok{'max_depth'}\NormalTok{: }\DecValTok{2}\NormalTok{, }\CharTok{'min_samples_leaf'}\NormalTok{: }\FloatTok{0.1}\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Then for each dictionary of hyperparams: - Train a model. - Use the
regular concordance index to compare their performances.\\
- Identify and return the best performing model.

     \#\#\# Exercise 10: hold out grid search

Implement hold out grid search.\\
\#\#\#\#\# Note In this case, you are not going to apply k-fold cross
validation. Since \texttt{sklearn.model\_selection.GridSearchCV()}
applies k-fold cross validation, you won't be using this to perform grid
search, and you will implement your own grid search.

Please see the hints if you get stuck.

     Hints

You can use the .items() or .values() method of a dictionary to get its
key, value pairs or just values. Use a list() to store them inside a
list.

To get all combinations of the hyperparams, you can use
itertools.product(*args\_list), where args\_list is a list object.

To generate the list of dictionaries, loop through the list of tuples.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}69}]:} \PY{c+c1}{\PYZsh{} UNQ\PYZus{}C10 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)}
         \PY{k}{def} \PY{n+nf}{holdout\PYZus{}grid\PYZus{}search}\PY{p}{(}\PY{n}{clf}\PY{p}{,} \PY{n}{X\PYZus{}train\PYZus{}hp}\PY{p}{,} \PY{n}{y\PYZus{}train\PYZus{}hp}\PY{p}{,} \PY{n}{X\PYZus{}val\PYZus{}hp}\PY{p}{,} \PY{n}{y\PYZus{}val\PYZus{}hp}\PY{p}{,} \PY{n}{hyperparam}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}}
         \PY{l+s+sd}{    Conduct hyperparameter grid search on hold out validation set. Use holdout validation.}
         \PY{l+s+sd}{    Hyperparameters are input as a dictionary mapping each hyperparameter name to the}
         \PY{l+s+sd}{    range of values they should iterate over. Use the cindex function as your evaluation}
         \PY{l+s+sd}{    function.}
         \PY{l+s+sd}{    }
         \PY{l+s+sd}{    Input:}
         \PY{l+s+sd}{        clf: sklearn classifier}
         \PY{l+s+sd}{        X\PYZus{}train\PYZus{}hp (dataframe): dataframe for training set input variables}
         \PY{l+s+sd}{        y\PYZus{}train\PYZus{}hp (dataframe): dataframe for training set targets}
         \PY{l+s+sd}{        X\PYZus{}val\PYZus{}hp (dataframe): dataframe for validation set input variables}
         \PY{l+s+sd}{        y\PYZus{}val\PYZus{}hp (dataframe): dataframe for validation set targets}
         \PY{l+s+sd}{        hyperparam (dict): hyperparameter dictionary mapping hyperparameter}
         \PY{l+s+sd}{                                                names to range of values for grid search}
         \PY{l+s+sd}{    }
         \PY{l+s+sd}{    Output:}
         \PY{l+s+sd}{        best\PYZus{}estimator (sklearn classifier): fitted sklearn classifier with best performance on}
         \PY{l+s+sd}{                                                                                 validation set}
         \PY{l+s+sd}{    \PYZsq{}\PYZsq{}\PYZsq{}}
             \PY{c+c1}{\PYZsh{} Initialize best estimator}
             \PY{n}{best\PYZus{}estimator} \PY{o}{=} \PY{k+kc}{None}
             
             \PY{c+c1}{\PYZsh{} initialize best hyperparam}
             \PY{n}{best\PYZus{}hyperparam} \PY{o}{=} \PY{p}{\PYZob{}}\PY{p}{\PYZcb{}}
             
             \PY{c+c1}{\PYZsh{} initialize the c\PYZhy{}index best score to zero}
             \PY{n}{best\PYZus{}score} \PY{o}{=} \PY{l+m+mf}{0.0}
             
             \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} START CODE HERE (REPLACE INSTANCES OF \PYZsq{}None\PYZsq{} with your code) \PYZsh{}\PYZsh{}\PYZsh{}}
             
             \PY{c+c1}{\PYZsh{} Get the values of the hyperparam and store them as a list of lists}
             \PY{n}{hyper\PYZus{}param\PYZus{}l} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{n}{hyperparam}\PY{o}{.}\PY{n}{values}\PY{p}{(}\PY{p}{)}\PY{p}{)}
             
             \PY{c+c1}{\PYZsh{} Generate a list of tuples with all possible combinations of the hyperparams}
             \PY{n}{combination\PYZus{}l\PYZus{}of\PYZus{}t} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{n}{itertools}\PY{o}{.}\PY{n}{product}\PY{p}{(}\PY{o}{*}\PY{n}{hyper\PYZus{}param\PYZus{}l}\PY{p}{)}\PY{p}{)}
             
             \PY{c+c1}{\PYZsh{} Initialize the list of dictionaries for all possible combinations of hyperparams}
             \PY{n}{combination\PYZus{}l\PYZus{}of\PYZus{}d} \PY{o}{=} \PY{p}{[}\PY{p}{]}
             
             \PY{c+c1}{\PYZsh{} loop through each tuple in the list of tuples}
             \PY{k}{for} \PY{n}{val\PYZus{}tuple} \PY{o+ow}{in} \PY{n}{combination\PYZus{}l\PYZus{}of\PYZus{}t}\PY{p}{:} \PY{c+c1}{\PYZsh{} complete this line}
                 \PY{n}{param\PYZus{}d} \PY{o}{=} \PY{p}{\PYZob{}}\PY{p}{\PYZcb{}}
                 
                 \PY{c+c1}{\PYZsh{} Enumerate each key in the original hyperparams dictionary}
                 \PY{k}{for} \PY{n}{i}\PY{p}{,} \PY{n}{k} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{hyperparam}\PY{o}{.}\PY{n}{keys}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{:} \PY{c+c1}{\PYZsh{} complete this line}
                     
                     \PY{c+c1}{\PYZsh{} add a key value pair to param\PYZus{}d for each value in val\PYZus{}tuple}
                     \PY{n}{param\PYZus{}d}\PY{p}{[}\PY{n}{k}\PY{p}{]} \PY{o}{=} \PY{n}{val\PYZus{}tuple}\PY{p}{[}\PY{n}{i}\PY{p}{]}
                 
                 \PY{c+c1}{\PYZsh{} append the param\PYZus{}dict to the list of dictionaries}
                 \PY{n}{combination\PYZus{}l\PYZus{}of\PYZus{}d}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{param\PYZus{}d}\PY{p}{)}
                 
             
             \PY{c+c1}{\PYZsh{} For each hyperparam dictionary in the list of dictionaries:}
             \PY{k}{for} \PY{n}{param\PYZus{}d} \PY{o+ow}{in} \PY{n}{combination\PYZus{}l\PYZus{}of\PYZus{}d}\PY{p}{:} \PY{c+c1}{\PYZsh{} complete this line}
                 
                 \PY{c+c1}{\PYZsh{} Set the model to the given hyperparams}
                 \PY{n}{estimator} \PY{o}{=} \PY{n}{clf}\PY{p}{(}\PY{o}{*}\PY{o}{*}\PY{n}{param\PYZus{}d}\PY{p}{)}
                 
                 \PY{c+c1}{\PYZsh{} Train the model on the training features and labels}
                 \PY{n}{estimator}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}hp}\PY{p}{,} \PY{n}{y\PYZus{}train\PYZus{}hp}\PY{p}{)}
                 
                 \PY{c+c1}{\PYZsh{} Predict the risk of death using the validation features}
                 \PY{n}{preds} \PY{o}{=} \PY{n}{estimator}\PY{o}{.}\PY{n}{predict\PYZus{}proba}\PY{p}{(}\PY{n}{X\PYZus{}val\PYZus{}hp}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}
                 
                 \PY{c+c1}{\PYZsh{} Evaluate the model\PYZsq{}s performance using the regular concordance index}
                 \PY{n}{estimator\PYZus{}score} \PY{o}{=} \PY{n}{concordance\PYZus{}index}\PY{p}{(}\PY{n}{y\PYZus{}val\PYZus{}hp}\PY{p}{,} \PY{n}{preds}\PY{p}{)}
                 
                 \PY{c+c1}{\PYZsh{} if the model\PYZsq{}s c\PYZhy{}index is better than the previous best:}
                 \PY{k}{if} \PY{n}{estimator\PYZus{}score} \PY{o}{\PYZgt{}} \PY{n}{best\PYZus{}score}\PY{p}{:} \PY{c+c1}{\PYZsh{} complete this line}
         
                     \PY{c+c1}{\PYZsh{} save the new best score}
                     \PY{n}{best\PYZus{}score} \PY{o}{=} \PY{n}{estimator\PYZus{}score}
                     
                     \PY{c+c1}{\PYZsh{} same the new best estimator}
                     \PY{n}{best\PYZus{}estimator} \PY{o}{=} \PY{n}{estimator}
                     
                     \PY{c+c1}{\PYZsh{} save the new best hyperparams}
                     \PY{n}{best\PYZus{}hyperparam} \PY{o}{=} \PY{n}{param\PYZus{}d}
                         
             \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} END CODE HERE \PYZsh{}\PYZsh{}\PYZsh{}}
         
             \PY{k}{if} \PY{n}{verbose}\PY{p}{:}
                 \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{hyperparam:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
                 \PY{n}{display}\PY{p}{(}\PY{n}{hyperparam}\PY{p}{)}
                 
                 \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{hyper\PYZus{}param\PYZus{}l}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
                 \PY{n}{display}\PY{p}{(}\PY{n}{hyper\PYZus{}param\PYZus{}l}\PY{p}{)}
                 
                 \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{combination\PYZus{}l\PYZus{}of\PYZus{}t}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
                 \PY{n}{display}\PY{p}{(}\PY{n}{combination\PYZus{}l\PYZus{}of\PYZus{}t}\PY{p}{)}
                 
                 \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{combination\PYZus{}l\PYZus{}of\PYZus{}d}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
                 \PY{n}{display}\PY{p}{(}\PY{n}{combination\PYZus{}l\PYZus{}of\PYZus{}d}\PY{p}{)}
                 
                 \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{best\PYZus{}hyperparam}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
                 \PY{n}{display}\PY{p}{(}\PY{n}{best\PYZus{}hyperparam}\PY{p}{)}
                 \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{best\PYZus{}score: }\PY{l+s+si}{\PYZob{}best\PYZus{}score:.4f\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
                 
             \PY{k}{return} \PY{n}{best\PYZus{}estimator}\PY{p}{,} \PY{n}{best\PYZus{}hyperparam}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}70}]:} \PY{c+c1}{\PYZsh{} Test}
         \PY{n}{n} \PY{o}{=} \PY{n}{X\PYZus{}dev}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
         \PY{n}{tmp\PYZus{}X\PYZus{}train} \PY{o}{=} \PY{n}{X\PYZus{}dev}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{p}{:}\PY{n+nb}{int}\PY{p}{(}\PY{n}{n}\PY{o}{*}\PY{l+m+mf}{0.8}\PY{p}{)}\PY{p}{,}\PY{p}{:}\PY{p}{]}
         \PY{n}{tmp\PYZus{}X\PYZus{}val} \PY{o}{=} \PY{n}{X\PYZus{}dev}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{n+nb}{int}\PY{p}{(}\PY{n}{n}\PY{o}{*}\PY{l+m+mf}{0.8}\PY{p}{)}\PY{p}{:}\PY{p}{,}\PY{p}{:}\PY{p}{]}
         \PY{n}{tmp\PYZus{}y\PYZus{}train} \PY{o}{=} \PY{n}{y\PYZus{}dev}\PY{p}{[}\PY{p}{:}\PY{n+nb}{int}\PY{p}{(}\PY{n}{n}\PY{o}{*}\PY{l+m+mf}{0.8}\PY{p}{)}\PY{p}{]}
         \PY{n}{tmp\PYZus{}y\PYZus{}val} \PY{o}{=} \PY{n}{y\PYZus{}dev}\PY{p}{[}\PY{n+nb}{int}\PY{p}{(}\PY{n}{n}\PY{o}{*}\PY{l+m+mf}{0.8}\PY{p}{)}\PY{p}{:}\PY{p}{]}
         
         \PY{n}{hyperparams} \PY{o}{=} \PY{p}{\PYZob{}}
             \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{n\PYZus{}estimators}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{20}\PY{p}{]}\PY{p}{,}
             \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{max\PYZus{}depth}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{]}\PY{p}{,}
             \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{min\PYZus{}samples\PYZus{}leaf}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mf}{0.1}\PY{p}{,} \PY{l+m+mf}{0.2}\PY{p}{]}\PY{p}{,}
             \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{random\PYZus{}state}\PY{l+s+s1}{\PYZsq{}} \PY{p}{:} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
         \PY{p}{\PYZcb{}}
         
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{ensemble} \PY{k}{import} \PY{n}{RandomForestClassifier}
         \PY{n}{control\PYZus{}model} \PY{o}{=} \PY{n}{holdout\PYZus{}grid\PYZus{}search}\PY{p}{(}\PY{n}{RandomForestClassifier}\PY{p}{,}
                                             \PY{n}{tmp\PYZus{}X\PYZus{}train}\PY{p}{,} \PY{n}{tmp\PYZus{}y\PYZus{}train}\PY{p}{,}
                                             \PY{n}{tmp\PYZus{}X\PYZus{}val}\PY{p}{,} \PY{n}{tmp\PYZus{}y\PYZus{}val}\PY{p}{,} \PY{n}{hyperparams}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
hyperparam:

    \end{Verbatim}

    
    \begin{verbatim}
{'n_estimators': [10, 20],
 'max_depth': [2, 5],
 'min_samples_leaf': [0.1, 0.2],
 'random_state': [0]}
    \end{verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]
hyper\_param\_l

    \end{Verbatim}

    
    \begin{verbatim}
[[10, 20], [2, 5], [0.1, 0.2], [0]]
    \end{verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]
combination\_l\_of\_t

    \end{Verbatim}

    
    \begin{verbatim}
[(10, 2, 0.1, 0),
 (10, 2, 0.2, 0),
 (10, 5, 0.1, 0),
 (10, 5, 0.2, 0),
 (20, 2, 0.1, 0),
 (20, 2, 0.2, 0),
 (20, 5, 0.1, 0),
 (20, 5, 0.2, 0)]
    \end{verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]
combination\_l\_of\_d

    \end{Verbatim}

    
    \begin{verbatim}
[{'n_estimators': 10,
  'max_depth': 2,
  'min_samples_leaf': 0.1,
  'random_state': 0},
 {'n_estimators': 10,
  'max_depth': 2,
  'min_samples_leaf': 0.2,
  'random_state': 0},
 {'n_estimators': 10,
  'max_depth': 5,
  'min_samples_leaf': 0.1,
  'random_state': 0},
 {'n_estimators': 10,
  'max_depth': 5,
  'min_samples_leaf': 0.2,
  'random_state': 0},
 {'n_estimators': 20,
  'max_depth': 2,
  'min_samples_leaf': 0.1,
  'random_state': 0},
 {'n_estimators': 20,
  'max_depth': 2,
  'min_samples_leaf': 0.2,
  'random_state': 0},
 {'n_estimators': 20,
  'max_depth': 5,
  'min_samples_leaf': 0.1,
  'random_state': 0},
 {'n_estimators': 20,
  'max_depth': 5,
  'min_samples_leaf': 0.2,
  'random_state': 0}]
    \end{verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]
best\_hyperparam

    \end{Verbatim}

    
    \begin{verbatim}
{'n_estimators': 10,
 'max_depth': 2,
 'min_samples_leaf': 0.1,
 'random_state': 0}
    \end{verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]
best\_score: 0.5928

    \end{Verbatim}

    T-Learner is a convenient framework because it does not restrict your
choice of base learners. - You will use random forests as the base
learners, but are able to choose another model as well.

    \hypertarget{expected-output}{%
\subparagraph{Expected output}\label{expected-output}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{hyperparam:}
\NormalTok{\{}\CharTok{'n_estimators'}\NormalTok{: [}\DecValTok{10}\NormalTok{, }\DecValTok{20}\NormalTok{],}
 \CharTok{'max_depth'}\NormalTok{: [}\DecValTok{2}\NormalTok{, }\DecValTok{5}\NormalTok{],}
 \CharTok{'min_samples_leaf'}\NormalTok{: [}\FloatTok{0.1}\NormalTok{, }\FloatTok{0.2}\NormalTok{],}
 \CharTok{'random_state'}\NormalTok{: [}\DecValTok{0}\NormalTok{]\}}
\NormalTok{hyper_param_l}
\NormalTok{[[}\AttributeTok{10}\NormalTok{, }\AttributeTok{20}\NormalTok{], [}\AttributeTok{2}\NormalTok{, }\AttributeTok{5}\NormalTok{], [}\AttributeTok{0}\NormalTok{.}\AttributeTok{1}\NormalTok{, }\AttributeTok{0}\NormalTok{.}\AttributeTok{2}\NormalTok{], [}\AttributeTok{0}\NormalTok{]]}
\DataTypeTok{combination_l_of_t}
\NormalTok{[(}\DecValTok{10}\NormalTok{, }\DecValTok{2}\NormalTok{, }\FloatTok{0.1}\NormalTok{, }\DecValTok{0}\NormalTok{),}
\NormalTok{ (}\DecValTok{10}\NormalTok{, }\DecValTok{2}\NormalTok{, }\FloatTok{0.2}\NormalTok{, }\DecValTok{0}\NormalTok{),}
\NormalTok{ (}\DecValTok{10}\NormalTok{, }\DecValTok{5}\NormalTok{, }\FloatTok{0.1}\NormalTok{, }\DecValTok{0}\NormalTok{),}
\NormalTok{ (}\DecValTok{10}\NormalTok{, }\DecValTok{5}\NormalTok{, }\FloatTok{0.2}\NormalTok{, }\DecValTok{0}\NormalTok{),}
\NormalTok{ (}\DecValTok{20}\NormalTok{, }\DecValTok{2}\NormalTok{, }\FloatTok{0.1}\NormalTok{, }\DecValTok{0}\NormalTok{),}
\NormalTok{ (}\DecValTok{20}\NormalTok{, }\DecValTok{2}\NormalTok{, }\FloatTok{0.2}\NormalTok{, }\DecValTok{0}\NormalTok{),}
\NormalTok{ (}\DecValTok{20}\NormalTok{, }\DecValTok{5}\NormalTok{, }\FloatTok{0.1}\NormalTok{, }\DecValTok{0}\NormalTok{),}
\NormalTok{ (}\DecValTok{20}\NormalTok{, }\DecValTok{5}\NormalTok{, }\FloatTok{0.2}\NormalTok{, }\DecValTok{0}\NormalTok{)]}
\NormalTok{combination_l_of_d}
\NormalTok{[\{}\CharTok{'n_estimators'}\NormalTok{: }\DecValTok{10}\NormalTok{,}
  \CharTok{'max_depth'}\NormalTok{: }\DecValTok{2}\NormalTok{,}
  \CharTok{'min_samples_leaf'}\NormalTok{: }\FloatTok{0.1}\NormalTok{,}
  \CharTok{'random_state'}\NormalTok{: }\DecValTok{0}\NormalTok{\},}
\NormalTok{ \{}\CharTok{'n_estimators'}\NormalTok{: }\DecValTok{10}\NormalTok{,}
  \CharTok{'max_depth'}\NormalTok{: }\DecValTok{2}\NormalTok{,}
  \CharTok{'min_samples_leaf'}\NormalTok{: }\FloatTok{0.2}\NormalTok{,}
  \CharTok{'random_state'}\NormalTok{: }\DecValTok{0}\NormalTok{\},}
\NormalTok{ \{}\CharTok{'n_estimators'}\NormalTok{: }\DecValTok{10}\NormalTok{,}
  \CharTok{'max_depth'}\NormalTok{: }\DecValTok{5}\NormalTok{,}
  \CharTok{'min_samples_leaf'}\NormalTok{: }\FloatTok{0.1}\NormalTok{,}
  \CharTok{'random_state'}\NormalTok{: }\DecValTok{0}\NormalTok{\},}
\NormalTok{ \{}\CharTok{'n_estimators'}\NormalTok{: }\DecValTok{10}\NormalTok{,}
  \CharTok{'max_depth'}\NormalTok{: }\DecValTok{5}\NormalTok{,}
  \CharTok{'min_samples_leaf'}\NormalTok{: }\FloatTok{0.2}\NormalTok{,}
  \CharTok{'random_state'}\NormalTok{: }\DecValTok{0}\NormalTok{\},}
\NormalTok{ \{}\CharTok{'n_estimators'}\NormalTok{: }\DecValTok{20}\NormalTok{,}
  \CharTok{'max_depth'}\NormalTok{: }\DecValTok{2}\NormalTok{,}
  \CharTok{'min_samples_leaf'}\NormalTok{: }\FloatTok{0.1}\NormalTok{,}
  \CharTok{'random_state'}\NormalTok{: }\DecValTok{0}\NormalTok{\},}
\NormalTok{ \{}\CharTok{'n_estimators'}\NormalTok{: }\DecValTok{20}\NormalTok{,}
  \CharTok{'max_depth'}\NormalTok{: }\DecValTok{2}\NormalTok{,}
  \CharTok{'min_samples_leaf'}\NormalTok{: }\FloatTok{0.2}\NormalTok{,}
  \CharTok{'random_state'}\NormalTok{: }\DecValTok{0}\NormalTok{\},}
\NormalTok{ \{}\CharTok{'n_estimators'}\NormalTok{: }\DecValTok{20}\NormalTok{,}
  \CharTok{'max_depth'}\NormalTok{: }\DecValTok{5}\NormalTok{,}
  \CharTok{'min_samples_leaf'}\NormalTok{: }\FloatTok{0.1}\NormalTok{,}
  \CharTok{'random_state'}\NormalTok{: }\DecValTok{0}\NormalTok{\},}
\NormalTok{ \{}\CharTok{'n_estimators'}\NormalTok{: }\DecValTok{20}\NormalTok{,}
  \CharTok{'max_depth'}\NormalTok{: }\DecValTok{5}\NormalTok{,}
  \CharTok{'min_samples_leaf'}\NormalTok{: }\FloatTok{0.2}\NormalTok{,}
  \CharTok{'random_state'}\NormalTok{: }\DecValTok{0}\NormalTok{\}]}
\NormalTok{best_hyperparam}
\NormalTok{\{}\CharTok{'n_estimators'}\NormalTok{: }\DecValTok{10}\NormalTok{,}
 \CharTok{'max_depth'}\NormalTok{: }\DecValTok{2}\NormalTok{,}
 \CharTok{'min_samples_leaf'}\NormalTok{: }\FloatTok{0.1}\NormalTok{,}
 \CharTok{'random_state'}\NormalTok{: }\DecValTok{0}\NormalTok{\}}
\NormalTok{best_score: }\FloatTok{0.5928}
\end{Highlighting}
\end{Shaded}

     \#\#\# Exercise 11: Training and validation, treatment and control
splits

\begin{itemize}
\tightlist
\item
  Unlike logistic regression, the machine learning algorithms used for
  base learners will generally require hyperparameter tuning, which
  means that you need to split your dev set into a training and
  validation set.
\item
  You need to also split each of the training and validation sets into
  \emph{treatment} and \emph{control} groups to train the treatment and
  control base learners of the T-Learner.
\end{itemize}

The function below takes in a dev dataset and splits it into training
and validation sets for treatment and control models, respectively.
Complete the implementation.

\hypertarget{note}{%
\paragraph{Note}\label{note}}

\begin{itemize}
\tightlist
\item
  The input X\_train and X\_val have the `TRTMT' column. Please remove
  the `TRTMT' column from the treatment and control features that the
  function returns.
\end{itemize}

     Hints

To drop a column, set the axis to 1 when calling
pandas.DataFrame.drop(\ldots{}). Axis=0 is used to drop a row by its
index label)

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}85}]:} \PY{c+c1}{\PYZsh{} UNQ\PYZus{}C11 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)}
         \PY{k}{def} \PY{n+nf}{treatment\PYZus{}dataset\PYZus{}split}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}val}\PY{p}{,} \PY{n}{y\PYZus{}val}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{    Separate treated and control individuals in training}
         \PY{l+s+sd}{    and testing sets. Remember that returned}
         \PY{l+s+sd}{    datasets should NOT contain the \PYZsq{}TRMT\PYZsq{} column!}
         
         \PY{l+s+sd}{    Args:}
         \PY{l+s+sd}{        X\PYZus{}train (dataframe): dataframe for subject in training set}
         \PY{l+s+sd}{        y\PYZus{}train (np.array): outcomes for each individual in X\PYZus{}train}
         \PY{l+s+sd}{        X\PYZus{}val (dataframe): dataframe for subjects in validation set}
         \PY{l+s+sd}{        y\PYZus{}val (np.array): outcomes for each individual in X\PYZus{}val}
         \PY{l+s+sd}{    }
         \PY{l+s+sd}{    Returns:}
         \PY{l+s+sd}{        X\PYZus{}treat\PYZus{}train (df): training set for treated subjects}
         \PY{l+s+sd}{        y\PYZus{}treat\PYZus{}train (np.array): labels for X\PYZus{}treat\PYZus{}train}
         \PY{l+s+sd}{        X\PYZus{}treat\PYZus{}val (df): validation set for treated subjects}
         \PY{l+s+sd}{        y\PYZus{}treat\PYZus{}val (np.array): labels for X\PYZus{}treat\PYZus{}val}
         \PY{l+s+sd}{        X\PYZus{}control\PYZus{}train (df): training set for control subjects}
         \PY{l+s+sd}{        y\PYZus{}control\PYZus{}train (np.array): labels for X\PYZus{}control\PYZus{}train}
         \PY{l+s+sd}{        X\PYZus{}control\PYZus{}val (np.array): validation set for control subjects}
         \PY{l+s+sd}{        y\PYZus{}control\PYZus{}val (np.array): labels for X\PYZus{}control\PYZus{}val}
         \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
             
             \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} START CODE HERE (REPLACE INSTANCES OF \PYZsq{}None\PYZsq{} with your code) \PYZsh{}\PYZsh{}\PYZsh{}}
             
             \PY{c+c1}{\PYZsh{} From the training set, get features of patients who received treatment}
             \PY{n}{X\PYZus{}treat\PYZus{}train} \PY{o}{=} \PY{n}{X\PYZus{}train}\PY{p}{[}\PY{n}{X\PYZus{}train}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{TRTMT}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{==} \PY{l+m+mi}{1}\PY{p}{]}
             
             \PY{c+c1}{\PYZsh{} drop the \PYZsq{}TRTMT\PYZsq{} column}
             \PY{n}{X\PYZus{}treat\PYZus{}train} \PY{o}{=} \PY{n}{X\PYZus{}treat\PYZus{}train}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{TRTMT}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{axis} \PY{o}{=} \PY{l+m+mi}{1}\PY{p}{)}
             
             \PY{c+c1}{\PYZsh{} From the training set, get the labels of patients who received treatment}
             \PY{n}{y\PYZus{}treat\PYZus{}train} \PY{o}{=} \PY{n}{y\PYZus{}train}\PY{p}{[}\PY{n}{X\PYZus{}train}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{TRTMT}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{==}\PY{l+m+mi}{1}\PY{p}{]}
         
             \PY{c+c1}{\PYZsh{} From the validation set, get the features of patients who received treatment}
             \PY{n}{X\PYZus{}treat\PYZus{}val} \PY{o}{=} \PY{n}{X\PYZus{}val}\PY{p}{[}\PY{n}{X\PYZus{}val}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{TRTMT}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{==} \PY{l+m+mi}{1}\PY{p}{]}
                                 
             \PY{c+c1}{\PYZsh{} Drop the \PYZsq{}TRTMT\PYZsq{} column}
             \PY{n}{X\PYZus{}treat\PYZus{}val} \PY{o}{=} \PY{n}{X\PYZus{}treat\PYZus{}val}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{TRTMT}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{axis} \PY{o}{=} \PY{l+m+mi}{1}\PY{p}{)}
                                 
             \PY{c+c1}{\PYZsh{} From the validation set, get the labels of patients who received treatment}
             \PY{n}{y\PYZus{}treat\PYZus{}val} \PY{o}{=} \PY{n}{X\PYZus{}train}\PY{p}{[}\PY{n}{X\PYZus{}train}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{TRTMT}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{==} \PY{l+m+mi}{1}\PY{p}{]}
                                 
         \PY{c+c1}{\PYZsh{} \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
                                 
             \PY{c+c1}{\PYZsh{} From the training set, get the features of patients who did not received treatment}
             \PY{n}{X\PYZus{}control\PYZus{}train} \PY{o}{=} \PY{n}{X\PYZus{}train}\PY{p}{[}\PY{n}{X\PYZus{}train}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{TRTMT}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{]}
                                 
             \PY{c+c1}{\PYZsh{} Drop the TRTMT column}
             \PY{n}{X\PYZus{}control\PYZus{}train} \PY{o}{=} \PY{n}{X\PYZus{}control\PYZus{}train}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{TRTMT}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{axis} \PY{o}{=} \PY{l+m+mi}{1}\PY{p}{)}
                                 
             \PY{c+c1}{\PYZsh{} From the training set, get the labels of patients who did not receive treatment}
             \PY{n}{y\PYZus{}control\PYZus{}train} \PY{o}{=} \PY{n}{y\PYZus{}train}\PY{p}{[}\PY{n}{X\PYZus{}train}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{TRTMT}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{]}
             
             \PY{c+c1}{\PYZsh{} From the validation set, get the features of patients who did not receive treatment}
             \PY{n}{X\PYZus{}control\PYZus{}val} \PY{o}{=} \PY{n}{X\PYZus{}val}\PY{p}{[}\PY{n}{X\PYZus{}val}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{TRTMT}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{]}
             
             \PY{c+c1}{\PYZsh{} drop the \PYZsq{}TRTMT\PYZsq{} column}
             \PY{n}{X\PYZus{}control\PYZus{}val} \PY{o}{=} \PY{n}{X\PYZus{}control\PYZus{}val}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{TRTMT}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{axis} \PY{o}{=} \PY{l+m+mi}{1}\PY{p}{)}
         
             \PY{c+c1}{\PYZsh{} From the validation set, get teh labels of patients who did not receive treatment}
             \PY{n}{y\PYZus{}control\PYZus{}val} \PY{o}{=} \PY{n}{y\PYZus{}val}\PY{p}{[}\PY{n}{X\PYZus{}val}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{TRTMT}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{]}
             
             \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} END CODE HERE \PYZsh{}\PYZsh{}\PYZsh{}}
         
             \PY{k}{return} \PY{p}{(}\PY{n}{X\PYZus{}treat\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}treat\PYZus{}train}\PY{p}{,}
                     \PY{n}{X\PYZus{}treat\PYZus{}val}\PY{p}{,} \PY{n}{y\PYZus{}treat\PYZus{}val}\PY{p}{,}
                     \PY{n}{X\PYZus{}control\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}control\PYZus{}train}\PY{p}{,}
                     \PY{n}{X\PYZus{}control\PYZus{}val}\PY{p}{,} \PY{n}{y\PYZus{}control\PYZus{}val}\PY{p}{)}
\end{Verbatim}


    \textbf{Test Case}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}86}]:} \PY{c+c1}{\PYZsh{} Tests}
         \PY{n}{example\PYZus{}df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{columns} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ID}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{TRTMT}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
         \PY{n}{example\PYZus{}df}\PY{o}{.}\PY{n}{ID} \PY{o}{=} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{100}\PY{p}{)}
         \PY{n}{example\PYZus{}df}\PY{o}{.}\PY{n}{TRTMT} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{binomial}\PY{p}{(}\PY{n}{n}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{p}\PY{o}{=}\PY{l+m+mf}{0.5}\PY{p}{,} \PY{n}{size}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{)}
         \PY{n}{treated\PYZus{}ids} \PY{o}{=} \PY{n+nb}{set}\PY{p}{(}\PY{n}{example\PYZus{}df}\PY{p}{[}\PY{n}{example\PYZus{}df}\PY{o}{.}\PY{n}{TRTMT}\PY{o}{==}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{ID}\PY{p}{)}
         \PY{n}{example\PYZus{}y} \PY{o}{=} \PY{n}{example\PYZus{}df}\PY{o}{.}\PY{n}{TRTMT}\PY{o}{.}\PY{n}{values}
         
         \PY{n}{example\PYZus{}train}\PY{p}{,} \PY{n}{example\PYZus{}val}\PY{p}{,} \PY{n}{example\PYZus{}y\PYZus{}train}\PY{p}{,} \PY{n}{example\PYZus{}y\PYZus{}val} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}
             \PY{n}{example\PYZus{}df}\PY{p}{,} \PY{n}{example\PYZus{}y}\PY{p}{,} \PY{n}{test\PYZus{}size} \PY{o}{=} \PY{l+m+mf}{0.25}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{0}
         \PY{p}{)}
         
         
         \PY{p}{(}\PY{n}{x\PYZus{}treat\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}treat\PYZus{}train}\PY{p}{,}
          \PY{n}{x\PYZus{}treat\PYZus{}val}\PY{p}{,} \PY{n}{y\PYZus{}treat\PYZus{}val}\PY{p}{,}
          \PY{n}{x\PYZus{}control\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}control\PYZus{}train}\PY{p}{,}
          \PY{n}{x\PYZus{}control\PYZus{}val}\PY{p}{,} \PY{n}{y\PYZus{}control\PYZus{}val}\PY{p}{)} \PY{o}{=} \PY{n}{treatment\PYZus{}dataset\PYZus{}split}\PY{p}{(}\PY{n}{example\PYZus{}train}\PY{p}{,} \PY{n}{example\PYZus{}y\PYZus{}train}\PY{p}{,}
                                                                  \PY{n}{example\PYZus{}val}\PY{p}{,} \PY{n}{example\PYZus{}y\PYZus{}val}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Tests}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{pass\PYZus{}flag} \PY{o}{=} \PY{k+kc}{True}
         \PY{n}{pass\PYZus{}flag} \PY{o}{=} \PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{x\PYZus{}treat\PYZus{}train}\PY{p}{)} \PY{o}{+} \PY{n+nb}{len}\PY{p}{(}\PY{n}{x\PYZus{}treat\PYZus{}val}\PY{p}{)} \PY{o}{+} \PY{n+nb}{len}\PY{p}{(}\PY{n}{x\PYZus{}control\PYZus{}train}\PY{p}{)} \PY{o}{+}
                      \PY{n+nb}{len}\PY{p}{(}\PY{n}{x\PYZus{}control\PYZus{}val}\PY{p}{)} \PY{o}{==} \PY{l+m+mi}{100}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{Didn}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{t lose any subjects: }\PY{l+s+si}{\PYZob{}pass\PYZus{}flag\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{pass\PYZus{}flag} \PY{o}{=} \PY{p}{(}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{TRTMT}\PY{l+s+s2}{\PYZdq{}} \PY{o+ow}{not} \PY{o+ow}{in} \PY{n}{x\PYZus{}treat\PYZus{}train}\PY{p}{)} \PY{o+ow}{and} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{TRTMT}\PY{l+s+s2}{\PYZdq{}} \PY{o+ow}{not} \PY{o+ow}{in} \PY{n}{x\PYZus{}treat\PYZus{}val}\PY{p}{)} \PY{o+ow}{and}
                      \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{TRTMT}\PY{l+s+s2}{\PYZdq{}} \PY{o+ow}{not} \PY{o+ow}{in} \PY{n}{x\PYZus{}control\PYZus{}train}\PY{p}{)} \PY{o+ow}{and} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{TRTMT}\PY{l+s+s2}{\PYZdq{}} \PY{o+ow}{not} \PY{o+ow}{in} \PY{n}{x\PYZus{}control\PYZus{}val}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{TRTMT not in any splits: }\PY{l+s+si}{\PYZob{}pass\PYZus{}flag\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{split\PYZus{}treated\PYZus{}ids} \PY{o}{=} \PY{n+nb}{set}\PY{p}{(}\PY{n}{x\PYZus{}treat\PYZus{}train}\PY{o}{.}\PY{n}{ID}\PY{p}{)}\PY{o}{.}\PY{n}{union}\PY{p}{(}\PY{n+nb}{set}\PY{p}{(}\PY{n}{x\PYZus{}treat\PYZus{}val}\PY{o}{.}\PY{n}{ID}\PY{p}{)}\PY{p}{)}
         \PY{n}{pass\PYZus{}flag} \PY{o}{=} \PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{split\PYZus{}treated\PYZus{}ids}\PY{o}{.}\PY{n}{union}\PY{p}{(}\PY{n}{treated\PYZus{}ids}\PY{p}{)}\PY{p}{)} \PY{o}{==} \PY{n+nb}{len}\PY{p}{(}\PY{n}{treated\PYZus{}ids}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{Treated splits have all treated patients: }\PY{l+s+si}{\PYZob{}pass\PYZus{}flag\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{split\PYZus{}control\PYZus{}ids} \PY{o}{=} \PY{n+nb}{set}\PY{p}{(}\PY{n}{x\PYZus{}control\PYZus{}train}\PY{o}{.}\PY{n}{ID}\PY{p}{)}\PY{o}{.}\PY{n}{union}\PY{p}{(}\PY{n+nb}{set}\PY{p}{(}\PY{n}{x\PYZus{}control\PYZus{}val}\PY{o}{.}\PY{n}{ID}\PY{p}{)}\PY{p}{)}
         \PY{n}{pass\PYZus{}flag} \PY{o}{=} \PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{split\PYZus{}control\PYZus{}ids}\PY{o}{.}\PY{n}{intersection}\PY{p}{(}\PY{n}{treated\PYZus{}ids}\PY{p}{)}\PY{p}{)} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{All subjects in control split are untreated: }\PY{l+s+si}{\PYZob{}pass\PYZus{}flag\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)} 
         \PY{n}{pass\PYZus{}flag} \PY{o}{=} \PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n+nb}{set}\PY{p}{(}\PY{n}{x\PYZus{}treat\PYZus{}train}\PY{o}{.}\PY{n}{ID}\PY{p}{)}\PY{o}{.}\PY{n}{intersection}\PY{p}{(}\PY{n}{x\PYZus{}treat\PYZus{}val}\PY{o}{.}\PY{n}{ID}\PY{p}{)}\PY{p}{)} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{No overlap between treat\PYZus{}train and treat\PYZus{}val: }\PY{l+s+si}{\PYZob{}pass\PYZus{}flag\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{pass\PYZus{}flag} \PY{o}{=} \PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n+nb}{set}\PY{p}{(}\PY{n}{x\PYZus{}control\PYZus{}train}\PY{o}{.}\PY{n}{ID}\PY{p}{)}\PY{o}{.}\PY{n}{intersection}\PY{p}{(}\PY{n}{x\PYZus{}control\PYZus{}val}\PY{o}{.}\PY{n}{ID}\PY{p}{)}\PY{p}{)} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{No overlap between control\PYZus{}train and control\PYZus{}val: }\PY{l+s+si}{\PYZob{}pass\PYZus{}flag\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZhy{}\PYZhy{}\PYZgt{} Expected: All statements should be True}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Tests

Didn't lose any subjects: True

TRTMT not in any splits: True

Treated splits have all treated patients: True

All subjects in control split are untreated: True

No overlap between treat\_train and treat\_val: True

No overlap between control\_train and control\_val: True

--> Expected: All statements should be True

    \end{Verbatim}

    You will now train a T-learner model on the patient data, and evaluate
its performance using the c-for-benefit.

First, get the training and validation sets.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}87}]:} \PY{c+c1}{\PYZsh{} Import the random forest classifier to be used as the base learner}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{ensemble} \PY{k}{import} \PY{n}{RandomForestClassifier}
         
         \PY{c+c1}{\PYZsh{} Split the dev data into train and validation sets}
         \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}val}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}val} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}\PY{n}{X\PYZus{}dev}\PY{p}{,} 
                                                           \PY{n}{y\PYZus{}dev}\PY{p}{,} 
                                                           \PY{n}{test\PYZus{}size} \PY{o}{=} \PY{l+m+mf}{0.25}\PY{p}{,}
                                                           \PY{n}{random\PYZus{}state} \PY{o}{=} \PY{l+m+mi}{0}\PY{p}{)}
\end{Verbatim}


    Split the training set into a treatment and control set.\\
Similarly, split the validation set into a treatment and control set.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}88}]:} \PY{c+c1}{\PYZsh{} get treatment and control arms of training and validation sets}
         \PY{p}{(}\PY{n}{X\PYZus{}treat\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}treat\PYZus{}train}\PY{p}{,} 
          \PY{n}{X\PYZus{}treat\PYZus{}val}\PY{p}{,} \PY{n}{y\PYZus{}treat\PYZus{}val}\PY{p}{,}
          \PY{n}{X\PYZus{}control\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}control\PYZus{}train}\PY{p}{,}
          \PY{n}{X\PYZus{}control\PYZus{}val}\PY{p}{,} \PY{n}{y\PYZus{}control\PYZus{}val}\PY{p}{)} \PY{o}{=} \PY{n}{treatment\PYZus{}dataset\PYZus{}split}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,}
                                                                  \PY{n}{X\PYZus{}val}\PY{p}{,} \PY{n}{y\PYZus{}val}\PY{p}{)}
\end{Verbatim}


    Choose a set of hyperparameters to perform grid search and find the best
model.\\
- Please first use these given hyperparameters so that you can get the
same c-for-benefit calculation at the end of this exercise. -
Afterwards, we encourage you to come back and try other ranges for these
hyperparameters.

\begin{Shaded}
\begin{Highlighting}[]
\ErrorTok{# Given hyperparams to do grid search}
\NormalTok{hyperparams = \{}
    \CharTok{'n_estimators'}\NormalTok{: [}\DecValTok{100}\NormalTok{, }\DecValTok{200}\NormalTok{],}
    \CharTok{'max_depth'}\NormalTok{: [}\DecValTok{2}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{40}\NormalTok{, None],}
    \CharTok{'min_samples_leaf'}\NormalTok{: [}\DecValTok{1}\NormalTok{, }\FloatTok{0.1}\NormalTok{, }\FloatTok{0.2}\NormalTok{],}
    \CharTok{'random_state'}\NormalTok{: [}\DecValTok{0}\NormalTok{]}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}89}]:} \PY{c+c1}{\PYZsh{} hyperparameter grid (we\PYZsq{}ll use the same one for both arms for convenience)}
         \PY{c+c1}{\PYZsh{} Note that we set random\PYZus{}state to zero}
         \PY{c+c1}{\PYZsh{} in order to make the output consistent each time it\PYZsq{}s run.}
         \PY{n}{hyperparams} \PY{o}{=} \PY{p}{\PYZob{}}
             \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{n\PYZus{}estimators}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mi}{100}\PY{p}{,} \PY{l+m+mi}{200}\PY{p}{]}\PY{p}{,}
             \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{max\PYZus{}depth}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{40}\PY{p}{,} \PY{k+kc}{None}\PY{p}{]}\PY{p}{,}
             \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{min\PYZus{}samples\PYZus{}leaf}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mf}{0.1}\PY{p}{,} \PY{l+m+mf}{0.2}\PY{p}{]}\PY{p}{,}
             \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{random\PYZus{}state}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
         \PY{p}{\PYZcb{}}
\end{Verbatim}


    Train the treatment base learner.\\
- Perform grid search to find a random forest classifier and associated
hyperparameters with the best c-index (the regular c-index).

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}95}]:} \PY{c+c1}{\PYZsh{} perform grid search with the treatment data to find the best model }
         \PY{n}{treatment\PYZus{}model}\PY{p}{,} \PY{n}{best\PYZus{}hyperparam\PYZus{}treat}  \PY{o}{=} \PY{n}{holdout\PYZus{}grid\PYZus{}search}\PY{p}{(}\PY{n}{RandomForestClassifier}\PY{p}{,}
                                               \PY{n}{X\PYZus{}treat\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}treat\PYZus{}train}\PY{p}{,}
                                               \PY{n}{X\PYZus{}treat\PYZus{}val}\PY{p}{,} \PY{n}{y\PYZus{}treat\PYZus{}val}\PY{p}{,} \PY{n}{hyperparams}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]

        ---------------------------------------------------------------------------

        ValueError                                Traceback (most recent call last)

        <ipython-input-95-ebddece39f65> in <module>()
          2 treatment\_model, best\_hyperparam\_treat  = holdout\_grid\_search(RandomForestClassifier,
          3                                       X\_treat\_train, y\_treat\_train,
    ----> 4                                       X\_treat\_val, y\_treat\_val, hyperparams)
    

        <ipython-input-69-51ab1569d514> in holdout\_grid\_search(clf, X\_train\_hp, y\_train\_hp, X\_val\_hp, y\_val\_hp, hyperparam, verbose)
         67 
         68         \# Evaluate the model's performance using the regular concordance index
    ---> 69         estimator\_score = concordance\_index(y\_val\_hp, preds)
         70 
         71         \# if the model's c-index is better than the previous best:


        /opt/conda/lib/python3.6/site-packages/lifelines/utils/concordance.py in concordance\_index(event\_times, predicted\_scores, event\_observed)
         52     """
         53     event\_times, predicted\_scores, event\_observed = \_preprocess\_scoring\_data(
    ---> 54         event\_times, predicted\_scores, event\_observed
         55     )
         56     num\_correct, num\_tied, num\_pairs = \_concordance\_summary\_statistics(event\_times, predicted\_scores, event\_observed)


        /opt/conda/lib/python3.6/site-packages/lifelines/utils/concordance.py in \_preprocess\_scoring\_data(event\_times, predicted\_scores, event\_observed)
        253 
        254     if event\_times.shape != predicted\_scores.shape:
    --> 255         raise ValueError("Event times and predictions must have the same shape")
        256     if event\_times.ndim != 1:
        257         raise ValueError("Event times can only be 1-dimensional: (n,)")


        ValueError: Event times and predictions must have the same shape

    \end{Verbatim}

    Train the control base learner.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}91}]:} \PY{c+c1}{\PYZsh{} perform grid search with the control data to find the best model }
         \PY{n}{control\PYZus{}model}\PY{p}{,} \PY{n}{best\PYZus{}hyperparam\PYZus{}ctrl} \PY{o}{=} \PY{n}{holdout\PYZus{}grid\PYZus{}search}\PY{p}{(}\PY{n}{RandomForestClassifier}\PY{p}{,}
                                             \PY{n}{X\PYZus{}control\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}control\PYZus{}train}\PY{p}{,}
                                             \PY{n}{X\PYZus{}control\PYZus{}val}\PY{p}{,} \PY{n}{y\PYZus{}control\PYZus{}val}\PY{p}{,} \PY{n}{hyperparams}\PY{p}{)}
\end{Verbatim}


    Combine the treatment and control base learners into the T-learner.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}94}]:} \PY{c+c1}{\PYZsh{} Save the treatment and control models into an instance of the TLearner class}
         \PY{n}{t\PYZus{}learner} \PY{o}{=} \PY{n}{TLearner}\PY{p}{(}\PY{n}{treatment\PYZus{}model}\PY{p}{,} \PY{n}{control\PYZus{}model}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]

        ---------------------------------------------------------------------------

        NameError                                 Traceback (most recent call last)

        <ipython-input-94-89da7ad10a94> in <module>()
          1 \# Save the treatment and control models into an instance of the TLearner class
    ----> 2 t\_learner = TLearner(treatment\_model, control\_model)
    

        NameError: name 'treatment\_model' is not defined

    \end{Verbatim}

    For the validation set, predict each patient's risk reduction.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{c+c1}{\PYZsh{} Use the t\PYZhy{}learner to predict the risk reduction for patients in the validation set}
        \PY{n}{rr\PYZus{}t\PYZus{}val} \PY{o}{=} \PY{n}{t\PYZus{}learner}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}val}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{TRTMT}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
        
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{X\PYZus{}val num of patients }\PY{l+s+si}{\PYZob{}X\PYZus{}val.shape[0]\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{rr\PYZus{}t\PYZus{}val num of patient predictions }\PY{l+s+si}{\PYZob{}rr\PYZus{}t\PYZus{}val.shape[0]\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    Now plot a histogram of your predicted risk reduction on the validation
set.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{n}{plt}\PY{o}{.}\PY{n}{hist}\PY{p}{(}\PY{n}{rr\PYZus{}t\PYZus{}val}\PY{p}{,} \PY{n}{bins}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{auto}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Histogram of Predicted ARR, T\PYZhy{}Learner, validation set}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{predicted risk reduction}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{count of patients}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    Notice when viewing the histogram that predicted risk reduction can be
negative. - This means that for some patients, the T-learner predicts
that treatment will actually increase their risk (negative risk
reduction). - The T-learner is more flexible compared to the logistic
regression model, which only predicts non-negative risk reduction for
all patients (view the earlier histogram of the `predicted ARR'
histogram for the logistic regression model, and you'll see that the
possible values are all non-negative).

    Now plot an empirical risk reduction plot for the validation set
examples.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{n}{empirical\PYZus{}benefit}\PY{p}{,} \PY{n}{avg\PYZus{}benefit} \PY{o}{=} \PY{n}{quantile\PYZus{}benefit}\PY{p}{(}\PY{n}{X\PYZus{}val}\PY{p}{,} \PY{n}{y\PYZus{}val}\PY{p}{,} \PY{n}{rr\PYZus{}t\PYZus{}val}\PY{p}{)}
        \PY{n}{plot\PYZus{}empirical\PYZus{}risk\PYZus{}reduction}\PY{p}{(}\PY{n}{empirical\PYZus{}benefit}\PY{p}{,} \PY{n}{avg\PYZus{}benefit}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{T Learner [val set]}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    Recall that the predicted risk reduction is along the horizontal axis
and the vertical axis is the empirical (actual risk reduction).

A good model would predict a lower risk reduction for patients with
actual lower risk reduction. Similarly, a good model would predict a
higher risk reduction for patients with actual higher risk reduction
(imagine a diagonal line going from the bottom left to the top right of
the plot).

The T-learner seems to be doing a bit better (compared to the logistic
regression model) at differentiating between the people who would
benefit most treatment and the people who would benefit least from
treatment.

    Compute the C-statistic-for-benefit on the validation set.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{n}{c\PYZus{}for\PYZus{}benefit\PYZus{}tlearner\PYZus{}val\PYZus{}set} \PY{o}{=} \PY{n}{c\PYZus{}statistic}\PY{p}{(}\PY{n}{rr\PYZus{}t\PYZus{}val}\PY{p}{,} \PY{n}{y\PYZus{}val}\PY{p}{,} \PY{n}{X\PYZus{}val}\PY{o}{.}\PY{n}{TRTMT}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{C\PYZhy{}for\PYZhy{}benefit statistic of T\PYZhy{}learner on val set: }\PY{l+s+si}{\PYZob{}c\PYZus{}for\PYZus{}benefit\PYZus{}tlearner\PYZus{}val\PYZus{}set:.4f\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \hypertarget{expected-output}{%
\subparagraph{Expected output}\label{expected-output}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{C-}\ControlFlowTok{for}\NormalTok{-benefit statistic of T-learner on val set: }\FloatTok{0.5043}
\end{Highlighting}
\end{Shaded}

    Now or the test set, predict each patient's risk reduction

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{c+c1}{\PYZsh{} predict the risk reduction for each of the patients in the test set}
        \PY{n}{rr\PYZus{}t\PYZus{}test} \PY{o}{=} \PY{n}{t\PYZus{}learner}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{TRTMT}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    Plot the histogram of risk reduction for the test set.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{c+c1}{\PYZsh{} Plot a histogram of the predicted risk reduction}
        \PY{n}{plt}\PY{o}{.}\PY{n}{hist}\PY{p}{(}\PY{n}{rr\PYZus{}t\PYZus{}test}\PY{p}{,} \PY{n}{bins}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{auto}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Histogram of Predicted ARR for the T\PYZhy{}learner on test set}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{predicted risk reduction}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{count of patients}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    Plot the predicted versus empircal risk reduction for the test set.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{c+c1}{\PYZsh{} Plot the predicted versus empirical risk reduction for the test set}
        \PY{n}{empirical\PYZus{}benefit}\PY{p}{,} \PY{n}{avg\PYZus{}benefit} \PY{o}{=} \PY{n}{quantile\PYZus{}benefit}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{rr\PYZus{}t\PYZus{}test}\PY{p}{)}
        \PY{n}{plot\PYZus{}empirical\PYZus{}risk\PYZus{}reduction}\PY{p}{(}\PY{n}{empirical\PYZus{}benefit}\PY{p}{,} \PY{n}{avg\PYZus{}benefit}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{T Learner (test set)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    Evaluate the T-learner's performance using the test set.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}93}]:} \PY{c+c1}{\PYZsh{} calculate the c\PYZhy{}for\PYZhy{}benefit of the t\PYZhy{}learner on the test set}
         \PY{n}{c\PYZus{}for\PYZus{}benefit\PYZus{}tlearner\PYZus{}test\PYZus{}set} \PY{o}{=} \PY{n}{c\PYZus{}statistic}\PY{p}{(}\PY{n}{rr\PYZus{}t\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{o}{.}\PY{n}{TRTMT}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{C\PYZhy{}for\PYZhy{}benefit statistic on test set: }\PY{l+s+si}{\PYZob{}c\PYZus{}for\PYZus{}benefit\PYZus{}tlearner\PYZus{}test\PYZus{}set:.4f\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]

        ---------------------------------------------------------------------------

        NameError                                 Traceback (most recent call last)

        <ipython-input-93-26a714882510> in <module>()
          1 \# calculate the c-for-benefit of the t-learner on the test set
    ----> 2 c\_for\_benefit\_tlearner\_test\_set = c\_statistic(rr\_t\_test, y\_test, X\_test.TRTMT)
          3 print(f"C-for-benefit statistic on test set: \{c\_for\_benefit\_tlearner\_test\_set:.4f\}")


        NameError: name 'rr\_t\_test' is not defined

    \end{Verbatim}

    \hypertarget{expected-output}{%
\subparagraph{Expected output}\label{expected-output}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{C-}\ControlFlowTok{for}\NormalTok{-benefit statistic on test set: }\FloatTok{0.5250}
\end{Highlighting}
\end{Shaded}

    The c-for-benefit of the two models were evaluated on different test
sets. However, we can compare their c-for-benefit scores to get a sense
of how they perform: - logistic regression: 0.5412 - T-learner: 0.5250

The T-learner doesn't actually do better than the logistic regression in
this case. You can try to tune the hyperparameters of the T-Learner to
see if you can improve it.

\hypertarget{note}{%
\subsubsection{Note}\label{note}}

While the more flexible ML techniques may improve predictive power, the
sample size is too small to be certain. - Models like the T-learner
could still be helpful in identifying subgroups who will likely not be
helped by treatment, or could even be harmed by treatment.\\
- So doctors can study these patients in more detail to find out how to
improve their outcomes.

    \hypertarget{congratulations}{%
\subsection{Congratulations}\label{congratulations}}

You've finished the assignment for Course 3 Module 1! We've seen that
machine learning techniques can help determine when a treatment will
have greater treatment effect for a particular patient.


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
